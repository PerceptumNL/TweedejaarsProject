\section{Experiments}
\subsection{Evaluation metrics}
In order to evalute the performance of the different algorithms, three different metrics were used. For each of these methods we hold on to the closed world assumption that if a link is not present within the given data set, it should not be a link. 

\subsubsection{Precision and recall}
Precision and recall can be calculated when the complete system pipeline is used. Precision reflects the fraction of relevant documents from all proposed documents and can thus be calculated as follows:

\begin{align*}
  \textrm{precision} = \frac{|\;\textrm{relevant\;documents} \cap \textrm{retrieved\;documents}\;|}{|\textrm{retrieved\;documents}\;|}
\end{align*}

Recall represents the fraction of relevant documents of all originally linked documents and can be calculated as follows:

\begin{align}
  \nonumber \textrm{recall} = \frac{|\;\textrm{relevant\;documents} \cap \textrm{retrieved\;documents}\;|}{|\;\textrm{relevant\;documents}\;|}
\end{align}
The precision and recall can be unraveled into precision and recall per document type to give more insight into the performance of the algorithms with regards to different document types. 

\subsubsection{K-links}
The k-links metric is used to evaluate the algorithms, without being influenced by the threshold for the number of proposed links. For a document with a given number of correct links, it proposes the same amount of links that the document is known to have. This evaluation metric thus makes the assumption that the algorithm knows in advance how many links should be returned. By doing so, the recall and precision are equivalent since the number of relevant and retrieved document is the same. It prevents the precision of being too optimistic, which would be the case if the fixed number would be lower than the actual amount of links. It also prevents the recall for being too optimistic in the cases that the actual amount of links is lower than the fixed number of proposed links. 

The disadvantage of the k-links metric is of course that it does not take into account the certainty the algorithm has due to the distances. For example, it could be that the distance of the first two ranked documents is very small, but the distance of the third is very large. If the original document has 10 links, the system is forced to additionally return the nine documents, even though these are likely to be wrong because they have a relatively big distance. 

\begin{table}

\begin{tabular}{| l | l | l | l | l | l | l | l |}
\hline
{\bf CORRELATION} & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 20.93 & 34.12 & 30.36 & 24.85 & 7.09 & 21.43 & {\bf 18.82} \\ 
Weighted text & 20.36 & 39.56 & 30.36 & 16.04 & 7.09 & 21.43 & {\bf 19.02} \\ 
Simple tag & 53.02 & 16.67 & 46.53 & 49.96 & 26.15 & 33.73 & {\bf 35.31} \\ 
Tag smoothing & 51.23 & 21.93 & 21.43 & 41.04 & 23.59 & 42.06 & {\bf 33.82} \\ 
Glossaries of tags & 15.03 & 15.35 & 32.14 & 25.73 & 7.95 & 35.71 & {\bf 14.52} \\ 
Weighted tag & 0 & 0 & 0 & 0 & 0 & 0 & {\bf 0} \\ 
\hline
\\
\hline
{\bf COSINE} & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 20.49 & 40.70 & 30.36 & 25.42 & 5.81 & 21.43 & {\bf 19.49} \\ 
Weighted text & 21.67 & 44.82 & 30.36 & 16.04 & 5.81 & 21.43 & {\bf 19.90} \\ 
Simple tag & 21.21 & 16.67 & 69.64 & 37.81 & 17.53 & 46.83 & {\bf 22.80 } \\ 
Tag smoothing & 20.58 & 21.93 & 44.64 & 31.04 & 13.59 & 46.83 & {\bf 20.69} \\ 
Glossaries of tags & 18.68 & 16.67 & 48.21 & 31.46 & 10.51 & 40.48 & {\bf 18.02} \\ 
Weighted tag & 0 & 0 & 0 & 0 & 0 & 0 & {\bf 0} \\ 
\hline
\\
\hline
{\bf INTERSECTION} & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 4.16 & 3.51 & 10.71 & 19.06 & 0 & 24.21 & {\bf 4.45} \\ 
Weighted text & 4.15 & 3.51 & 10.71 & 19.1 & 0.00 & 24.21 & {\bf 4.45} \\ 
Simple tag & 4.15 & 3.51 & 10.71 & 19.1 & 0.00 & 24.21 & {\bf 4.45} \\ 
Tag smoothing & 4.15 & 3.51 & 10.71 & 19.1 & 0.00 & 24.21 & {\bf 4.45} \\ 
Glossaries of tags & 4.16 & 3.51 & 10.71 & 19.06 & 0.00 & 24.21 & {\bf 4.45} \\ 
Weighted tag & 4.15 & 3.51 & 10.71 & 19.1 & 0.00 & 24.21 & {\bf 4.45} \\ 
\hline
\\
\hline
{\bf EUCLIDEAN} & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 3.04 & 3.95 & 17.86 & 0 & 0.85 & 21.43 & {\bf 3.25} \\ 
Tag smoothing & 1.32 & 1.32 & 7.14 & 0 & 0.43 & 0 & {\bf 1.08} \\ 
Simple tag & 14.05 & 10.53  & 30.36 & 38.23 & 14.36 & 35.71 & {\bf 16.56} \\ 
Tag smoothing & 0 & 0 & 0 & 0 & 0 & 0 & {\bf 0} \\ 
Glossaries of tags & 15.64 & 10.53 & 41.07 & 31.46 & 7.95 & 40.47 & {\bf 14.75} \\ 
Weighted tag & 0 & 0 & 0 & 0 & 0 & 0 & {\bf 0} \\ 
\hline
\\
\hline
{\bf BHATTACHARYYA} & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 15.57 & 35.88 & 30.36 & 16.15 & 5.30 & 21.43 & {\bf 16.19} \\ 
Weighted text & 18.72 & 29.04 & 30.35 & 11.99 & 7.95 & 21.43 & {\bf 16.63} \\ 
Simple tag & - & - & - & - & - & - & {\bf -} \\ 
Tag smoothing & 4.16 & 3.51 & 10.71 & 19.06 & 0 & 24.2 & {\bf 4.44} \\ 
Glossaries of tags & 18.36 & 17.98 & 48.21 & 48.13 & 10.51 & 40.45 & {\bf 19.39} \\ 
Weighted tag & 0 & 0 & 0 & 0 & 0 & 0 & {\bf 0} \\ 
\hline
\end{tabular}

\caption{Performance k-link measuring of all vectorizers with each of the distance metrics per document type}
\label{klink}
\end{table}

\subsection{Text-based descriptors}
Table \ref{klink} shows the k-link values of all vectorizers, including those of the textvectorizer and the weighted-text vectorizer. The best performance of the textvectorizers is obtained by the cosine distance. This can be attributed
 to the fact that the cosine distance is independent to document length and only
 computes a simmilarity in document structure. In other words when a document
 has the exact same words as a seconds document but only twice as much the
 cosine similarity classifies these documents as exactly equal. As the textvectorizer
 encodes the number of word occurences in a vector the cosine distance can 
 easily find document that use the same words frequently.

The cosine distance metric gives the best results for the text vectorisers. On average, 19.49\% and 19.59\% percent of the number of proposed links respectively are correct. The weighted text vectorizer performs a bit better, which is mainly due to an improvement in performance on information and questions. % ROBBERT, FIND SOME QUALTITATIVE EXAMPLES FOR THIS. 

Both textvectorizers have a low percentage correct with regards to proposing links for Persons. A further analysis shows that 76.36\% (not weighted) and 69.09\% (weighted) of the links for Persons are towards other Persons (see appendix X). However, within the Starfish network such links almost never occur (see table \ref{bayes_table3} for the distribution of document types within Starfish). This could explain the low performance on persons. 

Overall, both textvectorizers are slow in performance even though the corpus is small. Additionally, the the bag-of-words approach imposes a few limitations on the document linker. Firstly, it performs bad when different languages are used. Figure x shows the differences of vectors of three texts when an English document is combined with an English proposed document and a Dutch proposed document. In the case of two different languages, there are less words that the two documents have in common. If important keywords entail word such as 'clickers' versus 'stemsysteem', there is no way of relating the two documents. Secondly, the current StarFish network consists of mainly textual content. However, in the future this is likely to be extended with images, videos and other non-textual content. These sources should then somehow be converted to text.

\subsection{Tag-based descriptors}

\subsubsection{Simple tag similarity vectorizer}
The performance of the simple tag similarity vectorizer, as shown in table \ref{klink} together with the other tag vectorizers, is about 26\% precision when measuring k-link. The unraveling per document type shows that Question documents and Person documents perform the least. This can be explained by the fact that half of both Questions and Persons have zero tags. Obviously, the simple tag vectorizer cannot deal with such documents. In fact, almost all other Questions have only one tag. Since the simple tag vectorizer compares vectors, it wil prefer documents that also have only that particular tag, which makes it sensitive to attaching Questions to Questions. Something similar seems to happen with Persons, of which 50.91\% of the connections are with other Persons. Apparently, persons with similar expertise are tagged similarly. However, as mentioned with the text vectorizer, in Starfish persons almost never refer to other persons. Moreover, if a document is badly labeled this can also induce problems. For example, take the question 'Is there an English version in Tentamenlade', tagged with 'ToetsenEnToetsgestuurdleren'. The proposed links are visible in table \ref{proposed}, which shows that  the three proposed questions all have the tag 'ToetsEnToetsGestuudLeren'. However, if the question was tagged with the tag 'Tentamenlade', which seems very reasonable given the proposed question, the false negatives would likely be returned correctly by the system. Good practices, events and projects perform better, but these document types only entail 3.2\%, 2.7\% and 5.4\% of the total amount of documents respectively so have less influence on the average precision.

\begin{table}
\begin{lstlisting}
False Positives:
- Wat is het verschil tussen Learning Analytics en TTL (Question)
- Formatieve meerkeuze toetsen om begin kennisniveau te toetsen (Good Practice)
- De toetscyclus (Information)

True Positives:
- Tentamenlade2.5 (Project)

False Negatives:
- Tentamenlade Natuurkunde (Information)
- Hoe kan ik inloggen in Tentamenlade? (Question)
- Hoe kan ik inloggen in Tentamenlade? (Question)
\end{lstlisting}
\caption{Proposed links for the question 'Is there an English version in Tentamenlade?'}
\label{proposed}
\end{table}

\subsubsection{Tag smoothing}
The performance of the simple tag vectorizer, as shown in table \ref{klink}, is quite similar with the results of the tag similarity. It performs worse on the information, but better on questions. EXAMPLE THAT EXPLAINS WHY. It also performs worse on Persons. EXAMPLE THAT EXPLAINS WHY. 

In the current implementation this vectorizer is relatively slow. In practice the similarity matrix can be pre calculated and updated in batches. Due to the transform on the tag similartity matrix, it is very hard to determine which tag occurrences contributed to the document similarity and why some recommendations are made. It does not seem to perform much better than the regular bag of words tag descriptor, in \citeauthor{zhou2011web} the algorithm only starts performing significantly better when it is presented with more tags.

\subsubsection{Glossaries of tags}
To be continued.

\subsubsection{Hybrid}
Given the previous results, the best results can be obtained if the textvectorizer and simple tag similarity are combined into one vectorizer. If a document is a Question, it will be handeled by the cosine textvectorizer, otherwise the simple tag similarity with correlation will do this. The results are shown in table \ref{hybrid}. 

\begin{table}
\begin{tabular}{| l | l | l | l | l | l | l | l |}
\hline
 & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Accuracy hybrid & 53.02 & 40.70 & 46.43 & 43.96 & 26.15 & 33.73 & {\bf 39.58}\\
\hline
\end{tabular}
\caption{Accuracy of a hybrid form that uses the cosine textvectorizer for questions and the correlation simple tag similarity vectorizer otherwise}
\label{hybrid}
\end{table}

\subsection{Bayesian weighting}
\begin{table}
\begin{tabular}{| l | l | l | l | l | l | l | l |}
\hline
DEVALUATION & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 3.68 & 11.23 & 0.00 & 0.00 & 0.51 & 0.00 & {\bf 3.21}\\
Weighted text vectorizer & 3.60 & 11.23 & 0.00 & 0.00 & 0.51 & 0.00 & {\bf 3.19} \\ 
Simple tag similarity & 4.56 & 20.24 & 15.55 & 0 & 0.51 & 25.00 & {\bf 6.70}\\
Tag smoothing & 4.69 & 18.48 & 33.33 & 0.00 & 0.51 & 0.00 & {\bf 6.46}\\
Glossaries of tags & 5.35 & 20.24 & 0.00 & 3.93 & 8.21 & 25.00 & {\bf 9.02}\\
Weighted tag & 5.35 & 20.24 & 0.00 & 3.93 & 8.21 & 25.00 & {\bf 9.02}\\
\hline
\end{tabular}
\caption{Percentage correct links per vectorizer per document type after a k-link measurement whilst using tag and link devaluation NOT YET CORRECT VALUES!!!!}
\label{bayes_table1}
\end{table}

Table \ref{bayes_table1} shows the performance of each of the vectorizers (all with cosine distance) while applying the tag and link devaluation using the k-link metric. It shows that the performance of all algorithms drastically decreases when the probabilities are used to re-rank the documents. Table \ref{bayes_table1}  shows the distribution of links in the simple tag similarity vectorizer, with and without the probabilities (all based on k-link measuring). It is clear that the links with probabilties have a sharper distribution: the sparseness of the table shows that many types of links do not even exist. This effect could be caused by overfitting - the data set could be too small to calculate reliable probabilities. The prefered effect of having no Persons link to other Persons was done correctly. However, Good Pratices, for example, are now only assigned to be a person. The vectorizer without probabilities has a distribution that seems to be more reliable for these types of documents. The distributions with and without probabilities can be compared with the original distribution of links in the current Starfish knowledge base, as shown in table \ref{bayes_table2}.  

\begin{table}
\begin{tabular}{| l | l | l | l | l | l | l | }
\hline
DEVALUATION & {\bf Inf. }& {\bf Question }& {\bf Good Pr.} & {\bf Project }&{\bf Person }& {\bf Event}  \\
\hline
{\bf Information} & 82.79 &  01.08  &  0.00  &  0.00  & 16.12 & 0.00  \\
{\bf Question} & 12.22&  60.0  &  4.44 &  2.22  & 31.11 & 0.00 \\
{\bf Good Practice} & 41.18 &  41.18  &  0.00  &  5.88  & 11.76 & 0.00 \\
{\bf Project }& 56.67 &  20.0  &  0.00  &  10.00  & 13.33 & 0.00 \\
{\bf Person} & 81.82 &  3.64  &  1.82  &  0.00  & 12.73 & 0.00 \\
{\bf Event }& 35.71 &  0.00  &  14.29  &  0.00  & 50.00 & 0.00 \\
\hline
\\
\hline
NO DEVALUATION & {\bf Inf. }& {\bf Question }& {\bf Good Pr.} & {\bf Project }&{\bf Person }& {\bf Event} \\
\hline
{\bf Information} &  27.95 & 15.05 & 10.75 & 11.83 & 26.86 & 7.53 \\
{\bf Question} & 4.44 & 37.78 & 17.77 & 2.22 & 35.56 & 2.22\\
{\bf Good Practice} & 11.76 & 0 & 41.17 & 17.65 &17.65 & 5.88\\
{\bf Project } & 10.00 & 26.67 & 10.00 &16.67 & 26.67 &10.00 \\
{\bf Person} & 27.27 & 9.09 & 5.45 & 3.63 & 52.73 & 1.81 \\
{\bf Event }& 14.29 & 7.14 & 14.28 & 35.71 & 14.29 & 14.29 \\
\hline
\end{tabular}

\caption{Percentage of links from one type (row) to another (column) for simple\_tag\_vectorizer with tag and link devaluation (above) and without (below), measured using k-link. The rows sum up to 100\%.}
\label{bayes_table2}
\end{table}

\begin{table}
\begin{tabular}{| l | l | l | l | l | l | l | }
\hline
 & {\bf Inf. }& {\bf Question }& {\bf Good Pr.} & {\bf Project }&{\bf Person }& {\bf Event} \\
\hline
{\bf Information} &  39.86 & 8.39 &4.20 &5.59 &37.76 &4.20\\
{\bf Question} & 19.72 &25.35 &12.68 &12.68 &23.94 &5.63\\
{\bf Good Practice} & 25.00 & 17.86 & 7.14 & 10.71 & 25.00 & 14.29 \\
{\bf Project } & 23.91 & 10.87 & 4.35 & 19.57 & 30.43 & 10.87 \\
{\bf Person} & 69.64 & 8.93 & 1.79 & 8.93 & 1.79 & 8.93 \\
{\bf Event }& 28.57 & 0.00 & 14.29 & 9.52 & 33.33 & 14.29\\
\hline
\end{tabular}
\caption{Percentage of links from one type (row) to another (column) for the real links in the document base}
\label{bayes_table3}
\end{table}

Figure \ref{distribution} gives insight into the reason why the probabilities do not improve the performance. The figure shows a histogram of the percentage of documents that have a certain probability. The left side shows the distribution for the tag probabilities, where the red bars represent incorrect links and the green bars show the correct ones. One would expect that a higher percentage of correct links would be on the righthand side of the histogram, since these should have a higher probability. However, this is clearly not the case. On the contrary, about 75\% of the incorrect links have a chance of 0.0014, the highest probability. The link probability, shown on the right hand side of the figure, is a bit more promising since the incorrect links are a bit higher on the left hand side of the histogram. However, there is still no clear difference in distribution. 

\begin{figure}
\includegraphics[width =\textwidth]{images/probabilities}
\caption{The distribution in percentages of correct (green) and incorrect (red) document links. Left hand side shows the tag probabilities and the right hand side the link probabilities.}
\label{distribution}
\end{figure}


\subsection{Threshold performance}
\begin{table}
\begin{tabular}{| l | l | l | l | l | l | l | l |}
\hline
THRESHOLD RECALL & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 12.05 & 39.65 & 19.64 & 20.73 & 5.21 & 21.43 & {\bf 15.66}\\
Weighted text & 14.80 & 29.82 & 19.64 & 24.37 & 5.24 & 21.43 & {\bf 15.11}\\
Simple tag & 54.97 & 20.61 & 32.14 & 30.73 & 58.21 & 17.86 & {\bf 46.34}\\
Tag smoothing & 55.56 & 20.61 & 42.86 & 34.90 & 63.33 & 33.73 & {\bf 49.56}\\
Glossaries of tags & 36.51 & 23.25 & 21.43 & 36.35 & 50.20 & 44.05 & {\bf 38.80}\\
Weighted tag & 36.52 & 23.25 & 21.43 & 36.35 & 50.20 & 44.05 & {\bf 38.80}\\
Hybrid & 54.97 & 39.65 & 32.14 & 30.73 & 58.21 & 17.86 & {\bf 49.72}\\
\hline

\hline
THRESH PRECISION & Inf. &  Question &  Good Pr.& Project & Person &  Event & {\bf Average} \\
\hline
Textvectorizer & 26.47 & 50.00 & 41.67 & 24.79 & 24.79 & 7.26 & {\bf 24.05}\\
Weighted text & 26.76 & 41.67 & 33.33 & 26.25 & 8.55 & 27.78 & {\bf 23.00}\\
Simple Tag & 64.19 & 17.11 & 37.50 & 62.50 & 39.32 & 33.33 & {\bf 45.09}\\
Tag smoothing & 46.12 & 18.60 & 43.75 & 56.25 & 36.32 & 44.44 & {\bf 38.29}\\
Glossaries of tags & 25.00 & 14.47 & 37.50 & 33.18 & 17.26 & 46.67 & {\bf 22.00}\\
Weighted tag & 25.00 & 14.47 & 37.50 & 33.18 & 17.26 & 46.67 & {\bf 22.00}\\
Hybrid & 64.19 & 50.00 & 37.50 & 62.50 & 39.32 & 33.33 & {\bf 50.93}\\
\hline
\end{tabular}

\caption{Percentage correct links per vectorizer per document type after a k-link measurement}
\label{bayes_table1}
\end{table}

The performance of the automated threshold was measured for all vectorizers with the metric that gave the best results without threshold - cosine for the text-based descriptors and correlation for the tag-based descriptors. Now, precision and recall are different because the threshold can return a different number of links than the links that are known to be correct. Table xx shows the precision and recall for each of the vectorizers, including an unraveling for each type of document. A comparison between this table and table xx, which is measured with the k-link metric, shows that for the text-vectorizers performance decreases slightly when the threshold value is used. % Tell something about average number of differences in links & draw conclusions from unraveling
For the tag-based vectorizers, both precision and recall drastically improve. Note that precision and recall, with our way of measuring, the average precision and recall per document. Thus, the precision and recall can in fact improve because for some documents recall improves drastically and for some documents precision improves. This explains why the final performance of 46.3\% precision and 43.5\% recall for the simple text vectorizer can be reached. % Tell something about average number of differences in links & and draw conclusions from unraveling

In figure~\ref{fig:thresholds}, the distances for nearby documents the system recommends can be viewed. On the horizontal axis, the documents are shown in sorted order. The vertical axis displays the cosine distance for that document has from the new document. The patterns for all vectorizers seem very noisy, especially for documents that had very little links in the original document. The documents which had more links in the original dataset seem to be less scattered and more plausible to fit to a line.

\begin{figure}[h!]
\begin{tabular}{cc}
\includegraphics[width =0.5\textwidth]{images/thresh_cosine_glossaries_of_tags} 		& \includegraphics[width =0.5\textwidth]{images/thresh_cosine_textvectorizer} \\ \relax
\includegraphics[width =0.5\textwidth]{images/thresh_cosine_weighted_tagvectorizer}	& \includegraphics[width =0.5\textwidth]{images/thresh_cosine_weighted_text_vectorizer} \\ \relax
\includegraphics[width =0.5\textwidth]{images/thresh_cosine_simple_tag_similarity} 	& \includegraphics[width =0.5\textwidth]{images/thresh_cosine_tag_smoothing}
\end{tabular}
\caption{The sorted distances of nearest neighbors and their distance for differend vectorizers. The blue purple gradient represents documents with 0 links (blue) to $\>10$ links (purple)}
\label{fig:thresholds}
\end{figure}


\begin{figure}[h!]
\begin{tabular}{cc}
\includegraphics[width =0.5\textwidth]{images/thresh_cosine_glossaries_of_tags_distances} 		& \includegraphics[width =0.5\textwidth]{images/thresh_cosine_textvectorizer_distances} \\ \relax
\includegraphics[width =0.5\textwidth]{images/thresh_cosine_weighted_tagvectorizer_distances}	& \includegraphics[width =0.5\textwidth]{images/thresh_cosine_weighted_text_vectorizer_distances} \\ \relax
\includegraphics[width =0.5\textwidth]{images/thresh_cosine_simple_tag_similarity_distances} 	& \includegraphics[width =0.5\textwidth]{images/thresh_cosine_tag_smoothing_distances}
\end{tabular}
\caption{Distance betweeen two nearest neighbors sorted by document rank. The blue purple gradient represents documents with 0 links (blue) to $\>10$ links (purple)}
\label{fig:thresholds_differences}
\end{figure}
