\section{Method}

\subsection{Data preprocessing}
Starfish text content is serialized as HTML, a format which is not suitable for
calculating semantical similarity. Before processing, the data is sanitized by
removing HTML tags and entities and convert unicode characters to their closest
ASCII representation.


Maybe something about creating the folds by removing some of the documents?

\subsection{Text descriptors}
\subsubsection{Textvectorizer}

The first set of vectorizers focuses on the texts of the documents. The
\emph{textvectorizer} is a very generic approach that can be used on any corpus
of textual documents. In the Starfish context, we define 'content' as the title
and text-fields of a document. The only exception on this are Persons, of which
we wil use the name and about-fields. 

The textvectorizer first transforms the set of documents into a bag of words
and calculates the TF-IDF values for all words. This was done using the
TFIDFVectorizer of scikitlearn \citep{scikit-learn}.  Though the TF-IDF values
of words that are used very often should be low, common words such as 'and',
'or' and 'of' are still present in the vectors. This could be caused by the
different types of documents. For example, a Question often structured in a
less complex way than a Project description. To prevent this from happening,
the English stopword list that comes standard with scikit learn was used to
remove these words from the document descriptors.

\subsubsection{Weighted textvectorizer}
The weighted text vectorizer is an extension of the textvectorizer that takes
into account the links of the proposed documents. The vectors of the links of a
document are added with some weight to the vectors of the documents themselves.
Intuitively, this would add semantic information about a document based on it's
links. For example, a Person is likely to write other documents about his or
her subjects of expertise. Knowing not only the biography of a Person, but also
the content he or she has added to Starfish, gives a more complete image of
what documents could be related to that Person.

The vectors of links of a document are added in a recursive way, where
documents that are linked directly have a higher weight than documents that are
linked transitively. The algorithm is displayed in figure xx.

\subsection{Tag descriptors}
\subsubsection{Simple tag vectorizer}
The tag-based approach is more Starfish specific than the text-based approach,
since it depends on the tags that are available in Starfish. The tags on
Starfish are added by the users themselves, so offer a human-based vision on
what a document is really about. The simple tag vectorizer is a very straight
forward implementation of the idea of using tags. The vectors of this
transformation consist of a binary list that tells whether or not a tag is
attached to the document. 

\subsubsection{Tag smoothing}
The tag smoothing vectorizers creates descriptors based on the tag set of a
document. A tag co-occurs with other tags in a document, we assume documents
with similar tags should be linked in Starfish. Let the frequency of occurrence
with other tags across the dataset will form a vector for each tag. The
descriptor for a document is then created by combining the occurrence vectors
for all the document's tags. Now documents with tags that occur together will
be seen as similar.

There are two reasons why one would like to smooth the tag co-occurences.
Firstly, a problem for this is that tags must occur together before the
algorithms works properly. The Starfish dataset contains a lot of tags that
only occur with a small frequency, which means the tag occurrence vector will
contain many zeros. This makes the algorithm perform bad with little data.
Secondly, two tags can describe the same concept and be connected to that
concept through a common co-occurence with another tag. Whilst they describe
the same concept and are connected to that, they are not directly linked
together. Therefore it seems feasible to perform some sort of smoothing on the
co-occurences of tags.

\citet{zhou2011web} proposed a method to cluster web documents based on tag set
similarity. This is based on a similarity between two tags as a relation
between the frequency these tags occur separate and together, as described in
equation~\ref{eq:tag_similarity}. To smooth these similarities between tags, a
tag similarity matrix $\mathcal{C}$ is constructed. Each entry $c_{i,j}$ in
this matrix can be viewed as the angle $\theta_{i,j}$ between two unknown
vectors $v_i$ and $v_j$. These vectors cover both explicit similarity and
implicit similarity \citep{park2010vector}. This transfers the problem to find
a set of linearly independent vectors $\{v_1,v_2,\ldots,v_n\}$ for which for
all $v_i \cdot v_j = \cos(\theta_{i,j})$. One must find a matrix $\mathcal{V}$
for which $V^TV = C$. This can be done by orthogonal triangularization on
$\mathcal{C}$ for which \citeauthor{zhou2011web} introduces a modified Cholesky
transform.

\begin{equation} \label{eq:tag_similarity}
s_{i,j} = \frac{f_{i,j}}{f_i + f_j - f_{i,j}}
\end{equation}


\subsubsection{Glossaries of tags}
The glossaries of tags approach is also based on the intuition that certain
tags cover overlapping concepts. Just like the simple tag vectorizer the
glossaries of tags approach exploits this intuition. In the Starfish system a
tag is expected to have a glossary; a short English description of the concept
of a tag. These glossaries contain terms and words that are descriptive for the
tag. The glossary of tags vectorizer aims to use these terms to create a
document descriptor based on the tags associated with a document. In other
words the set of

First for each tag a tag descriptor $t_i$ is created. This tag descriptor is
a TF-IDF bag of words vector created from the glossaries of each tag. These
descriptors form the basis for the document descriptors. For each document
a document descriptor $d_i$ is created by summing the tag descriptors of the
associated tags.
\begin{align}
  d_i = \sum_{j \in T(d_i)} t_j
\end{align}
where $T(d_i)$ is the set of tag indices associated with document $d_i$. This
way a document descriptor is created based on a semantic representation of
the tags associated with the document.

\subsubsection{Weighted tag vectorizer}
The weighted tag vectorizer is similar to the glossaries of tags vectorizer
discussed in the previous section. However instead of just summing the tag
descriptors an weight is used to scale the tag descriptor. This way tags that
are associated with a lot of documents don't contribute as much to the document
descriptor as tags that are only associated with a few documents. The weight
$w_t$ for each tag descriptor $t$ is based on the number of documents that the
tag is associated with. $w_t$ is defined as follows
\begin{align}
  w_{t_i} = 1 - \frac{\textrm{Number of documents associated with $t_i$}}{\textrm{Total number of documents}}
\end{align}
With this weight the weighted tag document descriptor $d_i$ can be computed
in the following way:
\begin{align}
  d_i = \sum_{j \in T(d_i)} w_{t_j}t_j
\end{align}
This vectorizer first gives extra importance to descriptive words in each of
the tag descriptors by using the TF-IDF bag of words. After this more
importance is given to tags that are more descriptive using the weights. This
results in a document descriptor that has high weights for words that are
descriptive for tags that are descriptive for the document.

\subsection{Bayesian weighting}
Up to this point only the semantic similarity of documents has been taken into
account. This does however, not take the information in the current network
into account. For example the probability of a document of type information
linking to a document of type information is higher than that of the same
document linking to a document of type project. For this $P(d_i \to d_j \mid
\Sigma_{ij})$ and $P(d_i \to d_j)$ are interessting. Or in other words the
probability that two documents are linked given the number of tags they have in
common and the probability that two document of a given type are linked. These
probabilies can be estimated from the data. First it is shown how to compute
$P(d_i \to d_j \mid \Sigma_{ij})$.

\begin{align}
  P(d_i \to d_j \mid \Sigma_{ij}) &= \frac{P(\Sigma_{ij} \mid d_i \to d_j)P(d_i \to d_j)}{P(\Sigma_{ij})} \\
  P(\Sigma) &= \frac{{D_\Sigma \choose 2}}{{|D| \choose 2}} \\ 
  P(\Sigma \mid d_i \to d_j) &= \frac{\textrm{Number of links that have $\Sigma$ tags in common}}{|L|} \\
  P(d_i \to d_j) &= \frac{|L|}{{|D| \choose 2}}
\end{align}
Where $L$ is the set of all links and $D$ is the set of all documents.
$\Sigma_{ij}$ is the number of tags that document $d_i$ and $d_j$ have in
common. As is shown above $P(d_i \to d_j)$ is computed as a uniform probability
for all documents. However this can also be computer while taking the document
types into account. This is computed next:
\begin{align}
  P(d_i \to d_j) &= \frac{|L|}{{|D| \choose 2}}
\end{align}
These probabilities will be used to scale the distances that are
computed by the nearest neighbor algorithm. Because the distances for documents
that have a high probability should be low therefore the final distances are
computed in the following way:
\begin{align}
  d_i = (1 - P(d_i\to d_j \mid \Sigma_{ij})) \times (1-P(d_i \to d_j)) \times \delta_i
\end{align}
where $\delta_i$ is the distance as computed by the nearest neighbor algorithm
and $d_i$ the final distance that is used to select the proposed documents to
be linked. This may seem a rather simple statistical model, however due to the
limited set of items in the dataset it is very hard to learn a more complicated
model.

\subsection{Thresholds}
After creating the descriptors for a document and finding a new document's
nearest neighbors, a subpart of those links needs to be returned by the linker
application. An average document in Starfish has x links. To create a dynamic
threshold which returns similar a similar amount of documents, a document from
the Starfish data is extracted and then to re-inserted to test the number of
links the system returns to the number of links the document had in the
original system. Figure~\ref{fig:link_histogram} shows the distribution of
outgoing links in the current network.

\begin{figure}[h]
\centering
\includegraphics[width =0.45\textwidth]{images/link_histogram}
\caption{The frequency of outgoing links in Starfish documents}
\label{fig:link_histogram}
\end{figure}

The the threshold should depend on multiple factors which mostly depend on the setting in which the document linker will be used. Two major factors determine how the threshold should work.
\begin{enumerate}[1.]
	\item The degree of certainty we expect from the returned documents.
	\item The maximum number of documents that should be returned for a specific application.
\end{enumerate}
If the system is used to directly create the links into the Starfish system, the first aspect is very important. Only documents with a very high degree of linking certainty should then be returned, the amount should be based on the contents of the current system. If the system will be used to create recommendations which a user must accept or reject, the first aspect becomes less important and the system should return an amount of links which can quickly be reviewed by users. To ensure the flexibility for choice of algorithm and integration of the application, the threshold algorithm will contain a configurable parameter.

When adding a document to Starfish, it is assumed that there is always at least one related document. Based on the distance of the nearest neighbor for the new document, the index of a and the configurable parameter, the threshold is defined as in equation~\ref{eq:thresh}. In this equation, $\alpha$ is the configurable parameter, $m$ is the number of documents returned by the nearest neighbor algorithm, $d_0$ is the distance to the closest document, $\frac{m - n}{m}$ is a factor that ensures the maximum allowed distance decreases for documents ranked further away. This is based on the differences between two nearest neighbors which are visualized in figure~\ref{fig:thresholds_differences}. The distances have a long tail form: the distances between the nearest neighbors is relatively large for the closest documents and smaller between the documents further away. 

\begin{equation}
t_n = \alpha (1 - d_0) \frac{m - n}{m}
\label{eq:thresh}
\end{equation}
