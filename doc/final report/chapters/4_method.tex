\section{Method}
\subsection{Data preprocessing}
Starfish text content is serialized as HTML, a format which is not suitable for
calculating semantical similarity. Before processing, the data is sanitized by
removing HTML tags and entities and convert unicode characters to their closest
ASCII representation.

The raw starfish data contains tags which are aliases of other tags. During 
preprocessing all tags are replaced by their alias which have a glossary. Also, 
the current graph also contains some links which should not be returned by a 
link recommender system. Links from article to an author should be ignored as 
an author is automatically linked from the document. Links to glossaries should 
never occur, as they are linked through tags if appropriate.

After the data is sanitized, the tags are unique and the inappropriate links are 
removed, the data is ready for processing.

\subsection{Text descriptors}
\subsubsection{Textvectorizer}

The first set of vectorizers focuses on the texts of the documents. The
\emph{textvectorizer} is a very generic approach that can be used on any corpus
of textual documents. In the Starfish context, the content of a document is defined as the `title'
and `text'-fields of a document. The only exception on this are Persons, of which
the `name' and `about'-fields are used. 

The textvectorizer first transforms the set of documents into a bag of words
and calculates the TF-IDF values for all words. This was done using the
TFIDFVectorizer of scikitlearn \citep{scikit-learn}.  Though the TF-IDF values
of words that are used very often should be low, common words such as `and',
`or' and `of' are still present in the vectors. This could be caused by the
different types of documents. For example, a Question often structured in a
less complex way than a Project description. To prevent this from happening,
the English stopwords list that comes standard with scikit learn was used to
remove these words from the document descriptors.

\subsubsection{Weighted textvectorizer}
The \emph{weighted text vectorizer} is an extension of the textvectorizer that takes
into account the links of the proposed documents. The vectors of the links of a
document are added with some weight to the vectors of the documents themselves.
Intuitively, this would add semantic information about a document based on its
links. For example, a Person is likely to write other documents about his or
her subjects of expertise. Knowing not only the biography of a Person, but also
the content he or she has added to Starfish, gives a more complete image of
what documents could be related to that Person.

The vectors of links of a document are added in a recursive way, where
documents that are linked directly have a higher weight than documents that are
linked transitively. The recursive algorithm is displayed in algorithm~\ref{algo:wtv}.

\begin{algorithm}                      
  \caption{Weighted textvectorizer}      
  \label{algo:wtv}
  \begin{algorithmic}
    \STATE {\bf Function} weightedTextVectorize(document, depth, weight)
      \STATE descriptor = TF-IDF(document)
      \COMMENT{Convert document to vector}
      \IF{depth == 0}
        \RETURN descriptor
      \ENDIF
    \STATE linkdescriptor = $[0,0,\ldots,0]$
    \FOR{link $\in$ links(document)}
      \STATE linkdescriptor += weight $\times$ weightedTextVectorize(link, depth-1)
    \ENDFOR
    \STATE linkdescriptor = linkdescriptor / $|$links(document)$|$
    \COMMENT{Divide by the number of links}
    \RETURN descriptor + linkdescriptor
    \STATE {\bf EndFunction}
  \end{algorithmic}
\end{algorithm}

\subsection{Tag descriptors}
\subsubsection{Simple tag vectorizer}
The tag-based approach is more Starfish specific than the text-based approach,
since it depends on the tags that are available in Starfish. The tags on
Starfish are added by the users themselves, so offer a human-based vision on
what subject(s) a document really covers. The \emph{simple tag vectorizer} is a very
straightforward implementation of the idea of using tags. The vectors of this
transformation consist of a binary list that tells whether or not a tag is
attached to the document. 

\subsubsection{Tag smoothing}
The \emph{tag smoothing vectorizer} creates descriptors based on the tag set of a
document. If a tag co-occurs with other tags in a document, we assume that these
tags are similar. In this approach it is assumed that documents with such similar tags 
should be linked in Starfish. Let the frequency of occurrence
with other tags across the dataset form a vector for each tag. The
descriptor for a document is then created by combining the occurrence vectors
for all the document's tags. Now documents with tags that occur together will
be seen as similar.

There are two reasons why one would like to smooth the tag co-occurences.
Firstly, a problem for this is that tags must occur together before the
algorithms works properly. The Starfish dataset contains a lot of tags that
only occur with a small frequency, which means the tag occurrence vector will
contain many zeros. This makes the algorithm perform bad with little data.
Secondly, two tags can describe the same concept and be connected to that
concept through a common co-occurence with another tag. Whilst they describe
the same concept and are connected to that, they are not directly linked
together. Therefore it seems feasible to perform some sort of smoothing on the
co-occurences of tags.

\citet{zhou2011web} proposed a method to cluster web documents based on tag set
similarity. This is based on a similarity between two tags as a relation
between the frequency these tags occur separate and together, as described in
equation~\ref{eq:tag_similarity}. To smooth these similarities between tags, a
tag similarity matrix $\mathcal{C}$ is constructed. Each entry $c_{i,j}$ in
this matrix can be viewed as the angle $\theta_{i,j}$ between two unknown
vectors $v_i$ and $v_j$. These vectors cover both explicit similarity and
implicit similarity \citep{park2010vector}. This transfers the problem to find
a set of linearly independent vectors $\{v_1,v_2,\ldots,v_n\}$ for which for
all $v_i \cdot v_j = \cos(\theta_{i,j})$. One must find a matrix $\mathcal{V}$
for which $V^TV = C$. This can be done by orthogonal triangularization on
$\mathcal{C}$ for which \citeauthor{zhou2011web} introduces a modified Cholesky
transform.

\begin{equation} \label{eq:tag_similarity}
s_{i,j} = \frac{f_{i,j}}{f_i + f_j - f_{i,j}}
\end{equation}

\subsubsection{Glossaries of tags}
The \emph{glossaries of tags} approach is also based on the intuition that certain
tags cover overlapping concepts. Just like the simple tag vectorizer the
glossaries of tags approach exploits this intuition. In the Starfish system a
tag is expected to have a glossary; a short English description of the concept
of a tag. These glossaries contain terms and words that are descriptive for the
tag. The glossary of tags vectorizer aims to use these terms by creating a
document descriptor based on the descriptions of the tags that are associated 
with a document. 

First for each tag a tag descriptor $t_i$ is created. This tag descriptor is
a TF-IDF bag of words vector created from the glossaries of each tag. These
descriptors form the basis for the document descriptors. For each document
a document descriptor $d_i$ is created by summing the tag descriptors of the
associated tags:
\begin{align}
  d_i = \sum_{j \in T(d_i)} t_j
\end{align}
where $T(d_i)$ is the set of tag indices associated with document $d_i$. This
way a document descriptor is created based on a semantic representation of
the tags associated with the document.

\subsubsection{Weighted tag vectorizer}
The \emph{weighted tag vectorizer} is similar to the glossaries of tags vectorizer
discussed in the previous section. However instead of just summing the tag
descriptors an weight is used to scale the tag descriptor. This way tags that
are associated with a lot of documents don't contribute as much to the document
descriptor as tags that are only associated with a few documents. The weight
$w_t$ for each tag descriptor $t$ is based on the number of documents that the
tag is associated with. $w_t$ is defined as follows
\begin{align}
  w_{t_i} = 1 - \frac{\textrm{Number of documents associated with $t_i$}}{\textrm{Total number of documents}}
\end{align}
With this weight the weighted tag document descriptor $d_i$ can be computed
in the following way:
\begin{align}
  d_i = \sum_{j \in T(d_i)} w_{t_j}t_j
\end{align}
This vectorizer first gives extra importance to descriptive words in each of
the tag descriptors by using the TF-IDF bag of words. After this more
importance is given to tags that are more descriptive using the weights. This
results in a document descriptor that has high weights for words that are
descriptive for tags which themselves are descriptive for the document.

\subsubsection{Hybrid}
The \emph{hybrid} vectorizer is a hybrid of the textvectorizer and simple tag
similarity vectorizer. If a document has no tags, this vectorizer uses the textvectorizer.
Otherwise, the simple tag vectorizer is used. This prevents the simple tag vectorizer
from randomly assigning documents with one tag to a document that has no tags. 


\subsection{Bayesian weighting}
Up to this point only the semantic similarity of documents has been taken into
account. This does however, not take the information in the current network
into account. For example, the probability of a document of type information
linking to a document of type information is higher than that of the same
document linking to a document of type project. For this $P(d_i \to d_j \mid
\Sigma_{ij})$ and $P(d_i \to d_j)$ are interesting, which are the
probability that two documents are linked given the number of tags they have in
common and the probability that two document of a given type are linked, respectively. These
probabilies can be estimated from the data. First it is shown how to compute
$P(d_i \to d_j \mid \Sigma_{ij})$.

\begin{align}
  P(d_i \to d_j \mid \Sigma_{ij}) &= \frac{P(\Sigma_{ij} \mid d_i \to d_j)P(d_i \to d_j)}{P(\Sigma_{ij})} \\
  P(\Sigma) &= \frac{{D_\Sigma \choose 2}}{{|D| \choose 2}} \\ 
  P(\Sigma \mid d_i \to d_j) &= \frac{\textrm{Number of links that have $\Sigma$ tags in common}}{|L|} \\
  P(d_i \to d_j) &= \frac{|L|}{{|D| \choose 2}}
\end{align}
Where $L$ is the set of all links and $D$ is the set of all documents.
$\Sigma_{ij}$ is the number of tags that document $d_i$ and $d_j$ have in
common. As is shown above $P(d_i \to d_j)$ is computed as a uniform probability
for all documents. However, this can also be computed while taking the document
types into account. This results in a probability for every document type
to every document type. In total this are $\tau^2$ probabilities where $\tau$
is the number of document types.
\begin{align}
  P(d_i \to d_j) &= \frac{|\{(m, n) \mid m \to n \in L \land \tau(m) = \tau(d_i) \land \tau(n) = \tau(d_j)\}|}{|L|} \label{eq:linkprob}
\end{align}
Where $\tau(x)$ is the type of document $x$. In other words, the number of links
that have the type of $d_i$ as from document and the type of $d_j$ as to
document divided by the total number of links. The results of this for the
current network are displayed in table~\ref{table:linkprob}. This clearly shows
that a link from a type have preference to other types. This information should
be usefull when filtering bad links. These probabilities will be used to scale
the distances that are computed by the nearest neighbor algorithm. Because the
distances for documents that have a high probability should be low therefore
the final distances are computed in the following way:
\begin{align}
  d_i = (1 - P(d_i\to d_j \mid \Sigma_{ij})) \times (1-P(d_i \to d_j)) \times \delta_i 
\end{align}
where $\delta_i$ is the distance as computed by the nearest neighbor algorithm
and $d_i$ the final distance that is used to select the proposed documents to
be linked. This may seem a rather simple statistical model. However, due to the
limited set of items in the dataset it is very hard to learn a more complicated
model.


\begin{table}[h!]
\center
\begin{tabular}{l | l l l l l l l}
 & Informat & Glossary & Question & Good Pr. & Project & Person &  Event \\ \hline
Information      & 0.069544 & 0.057554 & 0.015588 & 0.008393 & 0.010791 & 0.065947 & 0.008393\\
Glossary         & 0.094724 & 0.052758 & 0.008393 & 0.011990 & 0.021583 & 0.070743 & 0.004796\\
Question         & 0.017986 & 0.013189 & 0.022782 & 0.011990 & 0.011990 & 0.021583 & 0.005995\\
Good Practice    & 0.009592 & 0.008393 & 0.007194 & 0.003597 & 0.004796 & 0.009592 & 0.005995\\
Project          & 0.014388 & 0.016787 & 0.007194 & 0.003597 & 0.011990 & 0.017986 & 0.007194\\
Person           & 0.092326 & 0.074341 & 0.017986 & 0.009592 & 0.019185 & 0.002398 & 0.011990\\
Event            & 0.008393 & 0.004796 & 0.001199 & 0.004796 & 0.003597 & 0.009592 & 0.00479 
\end{tabular}
\caption{The probability that a document of a type in the left column links to a document of a type in the top row as calculated using equation~\ref{eq:linkprob}.}
\label{table:linkprob}
\end{table}

\subsection{Thresholds}
After creating the descriptors for a document and finding a new document's
nearest neighbors, a subpart of those links needs to be returned by the linker
application. An average document in Starfish has 3.9 links. To create a dynamic
threshold which returns similar a similar amount of documents, a document from
the Starfish data is extracted and then to re-inserted to test the number of
links the system returns to the number of links the document had in the
original system. Figure~\ref{fig:link_histogram} shows the distribution of
outgoing links in the current network.

\begin{figure}[h]
\centering
\includegraphics[width =0.45\textwidth]{images/link_histogram}
\caption{The frequency of outgoing links in Starfish documents}
\label{fig:link_histogram}
\end{figure}

The threshold should depend on multiple factors which mostly depend on the setting in which the document linker will be used. Two major factors determine how the threshold should work.
\begin{enumerate}[1.]
	\item The degree of certainty we expect from the returned documents.
	\item The maximum number of documents that should be returned for a specific application.
\end{enumerate}
If the system is used to directly create the links into the Starfish system, the first aspect is very important. Only documents with a very high degree of linking certainty should then be returned. The amount should be based on the contents of the current system. If the system is used to create recommendations which a user must accept or reject, the first aspect becomes less important and the system should return an amount of links which can quickly be reviewed by users. To ensure the flexibility for choice of algorithm and integration of the application, the threshold algorithm will contain a configurable parameter.

When adding a document to Starfish, it is assumed that there is always at least one related document. Based on the distance of the nearest neighbor for the new document, the index of a document and the configurable parameter, the threshold, is defined as in equation~\ref{eq:thresh}. In this equation, $\alpha$ is the configurable parameter, $m$ is the number of documents returned by the nearest neighbor algorithm, $d_0$ is the distance to the closest document, $\frac{m - n}{m}$ is a factor that ensures the maximum allowed distance decreases for documents ranked further away. This is based on the differences between two nearest neighbors which are visualized in figure~\ref{fig:thresholds_differences}. The distances have a long tail form: the distances between the nearest neighbors is relatively large for the closest documents and smaller between the documents further away. 

\begin{equation}
t_n = \alpha (1 - d_0) \frac{m - n}{m}
\label{eq:thresh}
\end{equation}
