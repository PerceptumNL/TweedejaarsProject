\section{Conclusion}
This report has compared seven different ways (vectorizers) of converting a text
document into a vector representation and five ways of computing the distance
between these vectors. All this is done in the context of the Starfish network.
Also a method to take the knowledge from the network into account and a way to
determine the number of documents to retrieve were proposed. These combined
form a complete `pipeline' to compute and propose links to known documents for
a new document that is about to be added to the Starfish network.

The following conclusions can be drawn from the present study: firstly, the
text vectorizers performs the best if the newly added document is a Question (42.02-44.82\% accuracy on k-link measuring). However, it cannot deal with
documents that have different languages or non-textual documents such as images, videos
and audio. The simple tag vectorizer has the
best performance (22.80\% overall average accuracy on k-link measuring) and is the fastest. The best overall performance with the k-link measurement
is gained with the hybrid vectorizer (26.13\%) that uses the textvectorizer if no tags are
available and the simple tag vectorizer otherwise. This vectorizer performs as
good on most document types as the simple tag vectorizer, but performs significantly
better on questions (31.93\% versus 16.67\%). Secondly the probabilistic model of the network that is proposed 
is either to simplistic
or the data available is too little. In either case it might be off interest to
further investigate a similar model on a bigger data set. Lastly the method
of selecting the number of documents shows that the overall performance 
does not change significantly if the threshold is added to most vectorizers.
However, the text vectorizers seem to have a bias towards a higher precision in the
trade off between precision and recall. The
glossaries of tags and weighted glossaries of tags get a higher
recall for persons. The best performance while using the threshold was obtained
using the hybrid vectorizer, with an average precision, recall and F1 measure of respectively 28.61\%, 27.26\% and 23.92\%. 

The findings in this report are subject to at least three limitations. First,
the proposed solution only works for textual document and not on audio, video
which may be part of the Startfish knowledge graph. Secondly, the data set
that was used during investigation is rather small and may not be representation
of a real life data set. Lastly due to the small training network no cross-validation
was performed which may result in an overfit to the data.

Whilst this study is based on only a small network of documents taken together
these findings do show that the chosen solution is capable of recommending
links between documents with a certainty far above the guessing level. It is
now up to the client to choose if a precision of 28.61\%
is good enough to let the user select a document to link and if a recall of 27.26\%
covers enough of the documents within the knowledge base. 


