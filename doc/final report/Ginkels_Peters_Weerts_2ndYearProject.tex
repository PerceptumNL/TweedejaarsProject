\documentclass[a4paper]{article}

% Typesetting
%\usepackage{mathpazo}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=20,shrink=20]{microtype}
\microtypecontext{spacing=nonfrench}
\hyphenpenalty=5000
\tolerance=1000
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{tabularx} % in the preamble
\usepackage{setspace}


% Graphics
\usepackage{graphicx}
\graphicspath{{./images/}}

% Tables
\usepackage{booktabs}

% Utility
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumerate}
\usepackage{eulervm}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{color}
\usepackage{todonotes}

\renewcommand{\algorithmiccomment}[1]{\hfill// \textit{#1}}

\usepackage{framed, color}
\definecolor{shadecolor}{rgb}{0.8, 0.9, 1.0}

% Science Units
\usepackage{textcomp}
\usepackage[]{siunitx}
\sisetup{tophrase=--}
\sisetup{repeatunits=false}
\usepackage{latexsym}

% Bibliography (Use \citep)
\usepackage[]{natbib} %numbers
\setlength{\bibsep}{3.5pt}

\begin{document}

% Title Page
\newgeometry{margin=3cm}
\thispagestyle{empty}
\begin{center}
% StarFish Logo
\includegraphics[width=\linewidth]{logo_starfish_project}\\\vspace{1cm}

% Subtitle
{\large
Finding implicitly related items based on semantic similarities and metadata in a non-hierarchical network of documents
}\\\vspace{0.5cm}

% Authors
\begin{tabular*}{0.8\linewidth}{@{\extracolsep{\fill}}lcr}
& \textbf{Authors} & \\
\hline
R. van Ginkel 	& J. Peters 	& L. Weerts \\
\end{tabular*}
\\\vspace{2cm}
% Company
Project commissioned by \emph{Perceptum B.V.}\\\vspace{1cm}

% Supervisors
\textbf{Supervisors}\\
\begin{tabular}{r|l}
Academic Supervisor & Raquel Fernandez \\
Company Supervisors & Robrecht Jurriaans \\
& Sander Latour \\
& Wijnand Baretta
\end{tabular}


% UvA
\vfill
\today\\
Universiteit van Amsterdam

\end{center}
\restoregeometry

\clearpage

% Front Matter
\begin{abstract}
This report describes the results of the second year's project for the Perceptum team. The project focused on creating a \emph{document link recommender system} to the document knowledge base of the Starfish website. Because users do not have knowledge of all the documents in the entire knowledge base, an system that can perform this automatically is needed. The product created during performs this task using several different algorithms. 

To perform this task, a descriptor-based technique was used. Firstly, each of the documents is transformed into a descriptor by algorithms called \emph{vectorizers}. Two vectorizers based on the text of documents (textvectorizer and weighted text vectorizers), two others use the tags of documents (simple tag similarity and tag smoothing). The last two perform the textual transformation on text-based descriptions of tags (glossaries of tags and weighted tags). Secondly, a ranking is made of all documents based on the similarity of the document descriptors and the descriptor of the new added document. This was done using the Nearest Neighbor algorithm with several
distance metrics, of which cosine turned out to work the best. Then, it is possible to re-rank the proposed documents based on the probability that two documents are linked together, a number derived from the knowledge base distributions. The last step in the pipeline is to determine how many of the nearest neighbours should be returned. For this a threshold was set that compares the distance as calculated Nearest Neighbor of consecutive ranks. 

Three main conclusions can be drawn from this study. Firstly, the
text vectorizers perform significantly better for questions, however these do
perform badly on documents of type `person'. On documents of type `person' the
tag vectorizers do perform well. Suprisingly the simple tag vectorizer has the
best performance.  Secondly the probabilistic model of the network that is proposed 
is either to simplistic or the data available is too little. In either case it might be off interest to
further investigate a similar model on a bigger data set. Lastly the method
of selecting the number of documents shows that the precision and recall do
not change significantly if a threshold is added to most vectorizers. Only the
glossaries of tags and weighted glossaries of tags get a significantly higher
recall for persons. The best overall performance with the k-link measurement
is gained with the hybrid vectorizer that uses the textvectorizer if no tags are
available and the simple tag vectorizer otherwise. 

Whilst this study is based on only a small network of documents taken together
these findings do show that the chosen solution is capable of recommending
links between documents with a certainty far above the guessing level. It is
now up to the client to choose if a precision of 28.61\%
is good enough to let the user select a document to link and if a recall of 27.26\%
covers enough of the documents within the knowledge base. 
\end{abstract}
\clearpage

\tableofcontents
\clearpage

% Main Matter
% The introduction (dah)
\input{chapters/0_introduction}

% Domain (what are we dealing with and why is it non-trivial
\input{chapters/1_domain}

% Theory (how could this problem theoretically be solved)
\input{chapters/2_theory}

% What have we actually implemented
\input{chapters/3_productoverview}

% Very specific explanation of each of the algorithms in the pipeline and their pro's and cons
\input{chapters/4_method}

% Overview of experiments done on the entire pipeline
\input{chapters/5_experiments}

% Conclusion
\input{chapters/6_conclusion}

% MUCH FUTURE WORK!
\input{chapters/7_future}


% Marshall Matters
\bibliographystyle{apalike}
\bibliography{references.bib}


\end{document}
