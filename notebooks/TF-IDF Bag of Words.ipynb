{
 "metadata": {
  "name": "",
  "signature": "sha256:698bdb7f3b885d646f13fcd359236bd5aac5d0e30ab5f89c85900f2e2f2ad1e4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#TF-IDF Bag of Words\n",
      "\n",
      "To create the TF-IDF (Term Frequency, Inverse Document Frequency) bag of words we use the TfidVectorizer from scikit learn. This enables us to easily create the bag of words. First (for testing) we define a set of 'current' documents and a new document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = [\n",
      "    \"Het is een nacht die je normaal alleen in een film ziet\",\n",
      "    \"Het is een dag waarvan ik dacht ik dat ik hem\",\n",
      "    \"nooit beleven zou\",\n",
      "    \"maar vannacht beleef ik hem, met jou!\",\n",
      "]\n",
      "\n",
      "new_doc = \"Het is beleven dacht ik\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With these 'documents' we can setup the vectorizer and fit it to the set of documents. This lets the vectorizer learn a conversion law from the documents to array data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "import numpy as np\n",
      "\n",
      "vectorizer = TfidfVectorizer(use_idf=True)\n",
      "vectorizer.fit(docs)\n",
      "\n",
      "# quick test\n",
      "doc_tmp = 'Dit is een nacht waardoor ik nooit dacht'\n",
      "bag_of_words = vectorizer.transform([doc_tmp])\n",
      "print(bag_of_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 19)\t0.453386397373\n",
        "  (0, 18)\t0.453386397373\n",
        "  (0, 13)\t0.357455043342\n",
        "  (0, 11)\t0.357455043342\n",
        "  (0, 7)\t0.357455043342\n",
        "  (0, 3)\t0.453386397373\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "The output above is a sparse matrix repsentation of the bag of words that was created for the _doc_tmp_ we specified. Every line can be read as:\n",
      "    \n",
      "    (#document, word index) IDF\n",
      "    \n",
      "A complete bag of words is presented in the following block. Here every word in the bag of words representation gets a weight assigned based on the number of times the word occures in the document and in how many documents the words occures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(bag_of_words.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.          0.          0.          0.4533864   0.          0.          0.\n",
        "   0.35745504  0.          0.          0.          0.35745504  0.\n",
        "   0.35745504  0.          0.          0.          0.          0.4533864\n",
        "   0.4533864   0.          0.          0.          0.          0.        ]]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Generate BOW for all documents\n",
      "This bag of words representation can easily be generated for all know documents and new documents"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(vectorizer.transform(docs + [new_doc]).toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.29198411  0.          0.          0.          0.          0.\n",
        "   0.29198411  0.46040725  0.29198411  0.          0.23020362  0.\n",
        "   0.29198411  0.23020362  0.29198411  0.          0.          0.\n",
        "   0.29198411  0.          0.29198411  0.          0.          0.29198411\n",
        "   0.        ]\n",
        " [ 0.          0.          0.          0.28770928  0.28770928  0.28770928\n",
        "   0.          0.2268333   0.          0.2268333   0.2268333   0.6804999\n",
        "   0.          0.2268333   0.          0.          0.          0.          0.\n",
        "   0.          0.          0.          0.28770928  0.          0.        ]\n",
        " [ 0.          0.          0.57735027  0.          0.          0.          0.\n",
        "   0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.          0.          0.          0.57735027\n",
        "   0.          0.          0.          0.          0.57735027]\n",
        " [ 0.          0.40021825  0.          0.          0.          0.          0.\n",
        "   0.          0.          0.31553666  0.          0.31553666  0.          0.\n",
        "   0.          0.40021825  0.40021825  0.40021825  0.          0.          0.\n",
        "   0.40021825  0.          0.          0.        ]\n",
        " [ 0.          0.          0.50867187  0.50867187  0.          0.          0.\n",
        "   0.          0.          0.          0.40104275  0.40104275  0.\n",
        "   0.40104275  0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.          0.          0.        ]]\n"
       ]
      }
     ],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}