{
 "metadata": {
  "name": "",
  "signature": "sha256:93c8523c9ccf1f0558317f7a45698e3e965fb641f79a6e7fbac03f251285a2f3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import nltk\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import re\n",
      "from unidecode import unidecode\n",
      "import numpy as np\n",
      "from scipy import sparse\n",
      "import copy\n",
      "from decimal import *\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    with open('../data/export_starfish_tjp.pickle') as f:\n",
      "            data = pickle.load(f)\n",
      "    types = {}\n",
      "    for key in data['items']:\n",
      "        type = data['items'][key]['type']\n",
      "        if not types.has_key(type):\n",
      "            types[type] = data['items'][key].keys()\n",
      "    for key in types:\n",
      "        print(key)\n",
      "        print types[key]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Information\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Glossary\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Question\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Good Practice\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Project\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Person\n",
        "['about', 'name', 'links', 'tags', 'headline', 'type']\n",
        "Event\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Investigating fields in itemtypes\n",
      "\n",
      "This shows that all the types have 'text' and 'titles' which can be used for the algorithm. However, the Person does not have this. Therefore, we take the 'headline' and 'about' of the authors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import os, sys\n",
      "sys.path.append(os.path.join(os.path.abspath('..'), 'src'))  \n",
      "\n",
      "def find_links(data_, new_doc_idx):\n",
      "    # Remove a document from the dataset to use as 'new document'\n",
      "    data = copy.deepcopy(data_.data)\n",
      "    new_doc = data['items'][new_doc_idx]\n",
      "    del data['items'][new_doc_idx]\n",
      "    \n",
      "    # Get all texts for all documents\n",
      "    texts = dict(map(lambda x: (x[0], get_text(x[0], data_)), data['items'].items()))\n",
      "        \n",
      "def get_text(x):\n",
      "    data_.preprocessed_key(d)\n",
      "    if(x['type'] != 'Person'):\n",
      "         return x['title'] + ' ' + x['text']\n",
      "    else:\n",
      "         return x['headline'] + ' ' + x['about']\n",
      "'''\n",
      "def find_links(data_, new_doc_idx):\n",
      "    # Remove a document from the dataset to use as 'new document'\n",
      "    data = copy.deepcopy(data_)\n",
      "    new_doc = data['items'][new_doc_idx]\n",
      "    del data['items'][new_doc_idx]\n",
      "    doc_tags = map(lambda x: (x[0], x[1]['tags']), data['items'].items())\n",
      "\n",
      "    # Get all glossaries for all tags\n",
      "    texts = dict(map(lambda x: (x[0], glos(x)), data['tags'].items()))\n",
      "    vectorizer = TfidfVectorizer(use_idf=True)\n",
      "    vectorizer.fit(glossaries.values())\n",
      "    glossary_bows = vectorizer.transform(glossaries.values())\n",
      "    glossary_bows = dict(zip(glossaries.keys(), glossary_bows))\n",
      "\n",
      "    zero_vector = sparse.csc_matrix((1, len(vectorizer.get_feature_names())))\n",
      "    descriptors = []\n",
      "    for key, tags in doc_tags:\n",
      "        bows = map(lambda x: glossary_bows[x], tags)\n",
      "        descriptor = (sum(bows) + zero_vector) #/ float(len(tags) + 1)\n",
      "        descriptors += [(key, descriptor)]\n",
      "\n",
      "    # Get all tags for the new document\n",
      "    new_doc_descriptor = sum(map(lambda x: glossary_bows[x], new_doc['tags'])) + zero_vector\n",
      "\n",
      "    from sklearn.neighbors import NearestNeighbors\n",
      "\n",
      "    nn = NearestNeighbors()\n",
      "    nn.fit(sparse.vstack(dict(descriptors).values()))\n",
      "    dist, idx = nn.kneighbors(new_doc_descriptor, 40)\n",
      "\n",
      "    proposed_links = map(lambda x: descriptors[x][0], idx[0])\n",
      "\n",
      "    links = []\n",
      "    for link in new_doc['links']:\n",
      "        if link in proposed_links:\n",
      "            links += [(link, proposed_links.index(link))]\n",
      "        else:\n",
      "            links += [(link, None)]\n",
      "    return links\n",
      "'''\n",
      "\n",
      "data = DataWrapper('../data/export_starfish_tjp.pickle')\n",
      "print(find_links(data, 1))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}