{
 "metadata": {
  "name": "",
  "signature": "sha256:2d100b38e3026bdec33c392d072e8e9c2c901c54e495fc2ca689a16d540ec1e3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import nltk\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import re\n",
      "from unidecode import unidecode\n",
      "import numpy as np\n",
      "from scipy import sparse\n",
      "import copy\n",
      "from decimal import *\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    with open('../data/export_starfish_tjp.pickle') as f:\n",
      "            data = pickle.load(f)\n",
      "    types = {}\n",
      "    for key in data['items']:\n",
      "        type = data['items'][key]['type']\n",
      "        if not types.has_key(type):\n",
      "            types[type] = data['items'][key].keys()\n",
      "    for key in types:\n",
      "        print(key)\n",
      "        print types[key]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Information\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Glossary\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Question\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Good Practice\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Project\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Person\n",
        "['about', 'name', 'links', 'tags', 'headline', 'type']\n",
        "Event\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Investigating fields in itemtypes\n",
      "\n",
      "This shows that all the types have 'text' and 'titles' which can be used for the algorithm. However, the Person does not have this. Therefore, we take the 'headline' and 'about' of the authors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import os, sys\n",
      "sys.path.append(os.path.join(os.path.abspath('..'), 'src'))\n",
      "from datawrapper import DataWrapper\n",
      "\n",
      "def find_links(data_, new_doc_idx):\n",
      "    # Remove a document from the dataset to use as 'new document'\n",
      "    data = copy.deepcopy(data_.data)\n",
      "    new_doc = data['items'][new_doc_idx]\n",
      "    del data['items'][new_doc_idx]\n",
      "    \n",
      "    # Get all texts for all documents\n",
      "    texts = dict(map(lambda x: (x[0], get_text(x[0], data_)), data['items'].items()))\n",
      "        \n",
      "def get_text(x, data_):\n",
      "    return data_.preprocessed_text(x)\n",
      "'''\n",
      "    if(data_.item(x)['type'] != 'Person'):\n",
      "        #text = data_.preprocessed_key(x, 'text')\n",
      "        #title = data_.preprocessed_key(x, 'title')\n",
      "        return title + ' ' + title\n",
      "    else:\n",
      "        headline = data_.preprocessed_text_by_key(x, 'headline')\n",
      "        about = data_.preprocessed_text_by_key(x, 'about')\n",
      "        return headline + ' ' + about\n",
      "'''\n",
      "'''\n",
      "def find_links(data_, new_doc_idx):\n",
      "    # Remove a document from the dataset to use as 'new document'\n",
      "    data = copy.deepcopy(data_)\n",
      "    new_doc = data['items'][new_doc_idx]\n",
      "    del data['items'][new_doc_idx]\n",
      "    doc_tags = map(lambda x: (x[0], x[1]['tags']), data['items'].items())\n",
      "\n",
      "    # Get all glossaries for all tags\n",
      "    texts = dict(map(lambda x: (x[0], glos(x)), data['tags'].items()))\n",
      "    vectorizer = TfidfVectorizer(use_idf=True)\n",
      "    vectorizer.fit(glossaries.values())\n",
      "    glossary_bows = vectorizer.transform(glossaries.values())\n",
      "    glossary_bows = dict(zip(glossaries.keys(), glossary_bows))\n",
      "\n",
      "    zero_vector = sparse.csc_matrix((1, len(vectorizer.get_feature_names())))\n",
      "    descriptors = []\n",
      "    for key, tags in doc_tags:\n",
      "        bows = map(lambda x: glossary_bows[x], tags)\n",
      "        descriptor = (sum(bows) + zero_vector) #/ float(len(tags) + 1)\n",
      "        descriptors += [(key, descriptor)]\n",
      "\n",
      "    # Get all tags for the new document\n",
      "    new_doc_descriptor = sum(map(lambda x: glossary_bows[x], new_doc['tags'])) + zero_vector\n",
      "\n",
      "    from sklearn.neighbors import NearestNeighbors\n",
      "\n",
      "    nn = NearestNeighbors()\n",
      "    nn.fit(sparse.vstack(dict(descriptors).values()))\n",
      "    dist, idx = nn.kneighbors(new_doc_descriptor, 40)\n",
      "\n",
      "    proposed_links = map(lambda x: descriptors[x][0], idx[0])\n",
      "\n",
      "    links = []\n",
      "    for link in new_doc['links']:\n",
      "        if link in proposed_links:\n",
      "            links += [(link, proposed_links.index(link))]\n",
      "        else:\n",
      "            links += [(link, None)]\n",
      "    return links\n",
      "'''\n",
      "\n",
      "data = DataWrapper('../data/export_starfish_tjp.pickle')\n",
      "print vars(DataWrapper)\n",
      "print(find_links(data, 1))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'read_datafile': <function read_datafile at 0x10bfd8b90>, '__module__': 'datawrapper', 'tags': <property object at 0x10c0dbd08>, 'items': <property object at 0x10bfdd208>, 'tag': <function tag at 0x10bfd8de8>, 'item': <function item at 0x10bfd8ed8>, 'tag_glossary': <function tag_glossary at 0x10bfd82a8>, 'preprocessed_text': <function preprocessed_text at 0x10bfd8f50>, '__dict__': <attribute '__dict__' of 'DataWrapper' objects>, '__weakref__': <attribute '__weakref__' of 'DataWrapper' objects>, '__doc__': '\\n    This class wraps the data object exported from starfish.\\n    ', '__init__': <function __init__ at 0x10bfd8d70>}\n"
       ]
      },
      {
       "ename": "UnicodeDecodeError",
       "evalue": "'ascii' codec can't decode byte 0xc3 in position 1075: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-13-771b1bef61e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/export_starfish_tjp.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-13-771b1bef61e6>\u001b[0m in \u001b[0;36mfind_links\u001b[0;34m(data_, new_doc_idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get all texts for all documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-13-771b1bef61e6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get all texts for all documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-13-771b1bef61e6>\u001b[0m in \u001b[0;36mget_text\u001b[0;34m(x, data_)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m '''\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Person'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/lotte/tweedejaarsproject/TweedejaarsProject/src/datawrapper.py\u001b[0m in \u001b[0;36mpreprocessed_text\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'text'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/lotte/tweedejaarsproject/TweedejaarsProject/src/preprocessing/__init__.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m             markupMassage=hexentityMassage)\n\u001b[1;32m     23\u001b[0m     \u001b[0mtext_unicode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_unicode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc3 in position 1075: ordinal not in range(128)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}