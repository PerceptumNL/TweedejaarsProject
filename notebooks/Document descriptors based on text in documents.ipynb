{
 "metadata": {
  "name": "",
  "signature": "sha256:198c92d94f30a908c251c1767ff24b849318d456710e428e9a41679811caee20"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import nltk\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import re\n",
      "from unidecode import unidecode\n",
      "import numpy as np\n",
      "from scipy import sparse\n",
      "import copy\n",
      "from decimal import *\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    with open('../data/export_starfish_tjp.pickle') as f:\n",
      "            data = pickle.load(f)\n",
      "    types = {}\n",
      "    for key in data['items']:\n",
      "        type = data['items'][key]['type']\n",
      "        if not types.has_key(type):\n",
      "            types[type] = data['items'][key].keys()\n",
      "    for key in types:\n",
      "        print(key)\n",
      "        print types[key]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Information\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Glossary\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Question\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Good Practice\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Project\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n",
        "Person\n",
        "['about', 'name', 'links', 'tags', 'headline', 'type']\n",
        "Event\n",
        "['links', 'tags', 'text', 'author', 'title', 'type']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Investigating fields in itemtypes\n",
      "\n",
      "This shows that all the types have 'text' and 'titles' which can be used for the algorithm. However, the Person does not have this. Therefore, we take the 'headline' and 'about' of the authors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import os, sys\n",
      "sys.path.append(os.path.join(os.path.abspath('..'), 'src'))\n",
      "from datawrapper import DataWrapper\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "\n",
      "def find_links(data_, new_doc_idx):\n",
      "    # Remove a document from the dataset to use as 'new document'\n",
      "    data = copy.deepcopy(data_.data)\n",
      "    new_doc = data['items'][new_doc_idx]\n",
      "    \n",
      "    # Get all texts for all documents\n",
      "    texts = dict(map(lambda x: (x[0], get_text(x[0], data_)), data['items'].items()))\n",
      "    \n",
      "    # Vectorize texts\n",
      "    vectorizer = TfidfVectorizer(use_idf=True)\n",
      "    vectorizer.fit(texts.values())\n",
      "    bows = vectorizer.transform(texts.values())\n",
      "    bows = dict(zip(texts.keys(), bows))\n",
      "    \n",
      "    zero_vector = sparse.csc_matrix((1, len(vectorizer.get_feature_names())))    \n",
      "    descriptors = []\n",
      "    \n",
      "    for key in texts:\n",
      "        descriptor = bows[key] + zero_vector\n",
      "        descriptors += [(key, descriptor)]\n",
      "    \n",
      "    # Get all tags for the new document\n",
      "    new_doc_descriptor = bows[new_doc_idx] + zero_vector\n",
      "    \n",
      "    # Perform NN\n",
      "    nn = NearestNeighbors()\n",
      "    nn.fit(sparse.vstack(dict(descriptors).values()))\n",
      "    dist, idx = nn.kneighbors(new_doc_descriptor, 40)\n",
      "    \n",
      "    # Get proposed links\n",
      "    proposed_links = map(lambda x: descriptors[x][0], idx[0])\n",
      "\n",
      "    links = []\n",
      "    for link in new_doc['links']:\n",
      "        if link in proposed_links:\n",
      "            links += [(link, proposed_links.index(link))]\n",
      "        else:\n",
      "            links += [(link, None)]\n",
      "    return links\n",
      "\n",
      "def get_text(x, data_):\n",
      "    if(data_.item(x)['type'] != 'Person'):\n",
      "        text = data_.preprocessed_by_key(x, 'text')\n",
      "        title = data_.preprocessed_by_key(x, 'title')\n",
      "        return title + ' ' + text\n",
      "    else:\n",
      "        headline = data_.preprocessed_by_key(x, 'headline')\n",
      "        about = data_.preprocessed_by_key(x, 'about')\n",
      "        return headline + ' ' + about\n",
      "\n",
      "data = DataWrapper('../data/export_starfish_tjp.pickle')\n",
      "print(find_links(data, 3))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(6L, 25), (8L, None), (26L, 26), (52L, 29), (53L, None), (58L, None), (112L, 27), (157L, 30), (158L, None), (211L, None)]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Performing the algorithm\n",
      "\n",
      "For now the algorithm performs the following steps:\n",
      "1. For each document D in data (= data + new_doc), retrieve clean document text (= text + title or headline + about)\n",
      "2. Vectorize each text in data + new_doc\n",
      "3. Perform NN on the dataset - new_doc\n",
      "\n",
      "At a first glance I get the impression that this approach does not work so well"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Second version that takes weights of connections\n",
      "def find_links2(data_, new_doc_idx):\n",
      "    \n",
      "    # Remove a document from the dataset to use as 'new document'\n",
      "    data = copy.deepcopy(data_.data)\n",
      "    new_doc = data['items'][new_doc_idx]\n",
      "    \n",
      "    # Get all texts for all documents\n",
      "    texts = dict(map(lambda x: (x[0], get_text(x[0], data_)), data['items'].items()))\n",
      "    \n",
      "    # Vectorize texts\n",
      "    vectorizer = TfidfVectorizer(use_idf=True)\n",
      "    vectorizer.fit(texts.values())\n",
      "    bows = vectorizer.transform(texts.values())\n",
      "    bows = dict(zip(texts.keys(), bows))\n",
      "    \n",
      "    zero_vector = sparse.csc_matrix((1, len(vectorizer.get_feature_names())))    \n",
      "    descriptors = []\n",
      "    \n",
      "    for key in texts:\n",
      "        links = map(lambda x: bows[x], data['items'][key]['links'])\n",
      "        descriptor = (0.5*sum(links)/(len(links) + 1) + bows[key] + zero_vector)        \n",
      "        descriptors += [(key, descriptor)]\n",
      "    \n",
      "    # Get all tags for the new document\n",
      "    new_doc_descriptor = bows[new_doc_idx] + zero_vector\n",
      "    \n",
      "    # Perform NN\n",
      "    nn = NearestNeighbors()\n",
      "    nn.fit(sparse.vstack(dict(descriptors).values()))\n",
      "    dist, idx = nn.kneighbors(new_doc_descriptor, 40)\n",
      "    \n",
      "    # Get proposed links\n",
      "    proposed_links = map(lambda x: descriptors[x][0], idx[0])\n",
      "\n",
      "    links = []\n",
      "    for link in new_doc['links']:\n",
      "        if link in proposed_links:\n",
      "            links += [(link, proposed_links.index(link))]\n",
      "        else:\n",
      "            links += [(link, None)]\n",
      "    return links\n",
      "\n",
      "print(find_links2(data, 5))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(11L, 30)]\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example of a weighted descriptor that takes the discriptors of other documents into regard, by taking the average weights of all it's linked documents and add these with 0.5 weight"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}