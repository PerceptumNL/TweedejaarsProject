{
 "metadata": {
  "name": "",
  "signature": "sha256:d99e807a565de1b7f2fe142b036d55d799751ff60a1c4b75af8ba607b17363fe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Generate TF-IDF descriptors for documents\n",
      "\n",
      "On a high level descriptors of a document are created by summing (and potentially normalizing) the TF-IDF bag of word vectors of the glossaries of the tags that are associated with the document.\n",
      "\n",
      "\\begin{align}\n",
      "K &= \\text{Set of all known documents $k$} \\\\\n",
      "d &= \\text{New document to add to graph} \\\\\n",
      "T & = \\text{Set of all tags $t$} \\\\\n",
      "\\end{align}\n",
      "\n",
      "We then need a way the get all the tags of a document and a definition for the descriptor.\n",
      "\n",
      "\\begin{align}\n",
      "T(k) &= \\{t \\mid t \\in T \\land t = \\text{tag of k}\\} \\\\ \n",
      "des(k) &= \\frac{1}{|T(k)|}\\sum_{t \\in T(K)}\\text{TF-IDF}(t)\n",
      "\\end{align}\n",
      "\n",
      "Where $\\text{TF-IDF}(t)$ is the TF-IDF bag of words representation of the glossary of tag $t$. With this and the TF-IDF generation described in the notebook 'TF-IDF Bag of Words' we are ready to generate the descriptors for the documents.\n",
      "\n",
      "## Descriptors for starfish data\n",
      "\n",
      "First we load the data and find all tags associated with the documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "\n",
      "with open('../data/export_starfish_tjp.pickle') as f:\n",
      "    data = pickle.load(f)\n",
      "\n",
      "# Remove a document from the dataset to use as 'new document'\n",
      "new_doc = data['items'][157]\n",
      "del data['items'][157]\n",
      "\n",
      "doc_tags = map(lambda x: (x[0], x[1]['tags']), data['items'].items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 201
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then get all glossaries for all tags. The tags that have no glossary will get an empty string assigned as glossary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import re\n",
      "from unidecode import unidecode\n",
      "\n",
      "def preprocess(text):\n",
      "    \"\"\"\n",
      "    Preproccess text. This is described in the 'preprocessing' notebook.\n",
      "    \"\"\"\n",
      "    html_stripped = nltk.clean_html(text)\n",
      "    # regex to also decode hex entities\n",
      "    hexentityMassage = [(re.compile('&#x([^;]+);'), lambda m: '&#%d;' % int(m.group(1), 16))]\n",
      "    glossary_tag = BeautifulSoup(html_stripped, convertEntities=BeautifulSoup.HTML_ENTITIES, markupMassage=hexentityMassage)\n",
      "    glossary_unicode = glossary_tag.text.encode('utf-8')\n",
      "    return unidecode(glossary_unicode)\n",
      "    \n",
      "def glos(tag):\n",
      "    \"\"\"\n",
      "    Given a tag return the preprocessed glossary\n",
      "    \"\"\"\n",
      "    if tag[1]['glossary'] is not None:\n",
      "        return preprocess(data['items'][tag[1]['glossary']]['text'])\n",
      "    else:\n",
      "        return ''\n",
      "\n",
      "# Get all glossaries for all tags\n",
      "glossaries = dict(map(lambda x: (x[0], glos(x)), data['tags'].items()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With these glossaries we are ready to generate the TF-IDF bag of word representations for each tag."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "vectorizer = TfidfVectorizer(use_idf=True)\n",
      "vectorizer.fit(glossaries.values())\n",
      "glossary_bows = vectorizer.transform(glossaries.values())\n",
      "glossary_bows = dict(zip(glossaries.keys(), glossary_bows))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 203
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the bag of words representation for each tag we can create the descriptor ($des(k)$) for all documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import sparse\n",
      "\n",
      "zero_vector = sparse.csc_matrix((1, len(vectorizer.get_feature_names())))\n",
      "descriptors = []\n",
      "for key, tags in doc_tags:\n",
      "    bows = map(lambda x: glossary_bows[x], tags)\n",
      "    descriptor = (sum(bows) + zero_vector) #/ float(len(tags) + 1)\n",
      "    descriptors += [(key, descriptor)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 204
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also need to create a descriptor for the new document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all tags for the new document\n",
      "new_doc_descriptor = sum(map(lambda x: glossary_bows[x], new_doc['tags'])) + zero_vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 212
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the descriptor for the new document and the descriptors for all known documents we are ready to run the nearest neighbors algorithm to find the best matching documents for the new document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import NearestNeighbors\n",
      "\n",
      "nn = NearestNeighbors()\n",
      "nn.fit(sparse.vstack(dict(descriptors).values()))\n",
      "dist, idx = nn.kneighbors(new_doc_descriptor, 40)\n",
      "\n",
      "proposed_links = map(lambda x: descriptors[x][0], idx[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 262
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The nearest neighbor algorithm returns the fourty best links based on the descriptors and the default distance metric. We can comparse these proposed links to the hand created links in the data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for link in new_doc['links']:\n",
      "    if link in proposed_links:\n",
      "        print(link, proposed_links.index(link))\n",
      "    else:\n",
      "        print(link, None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1L, None)\n",
        "(3L, 3)\n",
        "(6L, 4)\n",
        "(52L, 0)\n",
        "(77L, None)\n",
        "(108L, 16)\n",
        "(112L, 5)\n",
        "(150L, 17)\n"
       ]
      }
     ],
     "prompt_number": 266
    }
   ],
   "metadata": {}
  }
 ]
}