{
 "metadata": {
  "name": "",
  "signature": "sha256:2d9b64bb25df61643496c4ae9e874c116feaaa6b2ede2e436c715ef530c7f239"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Bv. tentamenlade bij Wat is het verschil tussen Learning Analytics en TTL, puur omdat die ook over TTL gaat \n",
      "\n",
      "Waarop worden de links gebaseerd als iets geen tags heeft? Bv. 'zijn er nieuwe voorbeelden bij de FNWI over peer instruction?'\n",
      "\n",
      "Projecten komen veelal bij elkaar omdat ze over dezelfde onderwerpen gaan, maar zijn niet per se relevant\n",
      "\n",
      "Engels en Nederands gaan goed samen omdat die 1 tag hebben\n",
      "\n",
      "ALGEMEEN: Is het niet raar om te claimen dat we het de user makkelijker maken door te zeggen dat we documenten aanbieden, maar tegelijkertijd wel eisen dat hij ze getagt heeft???\n",
      "\n",
      "\n",
      "Bv. tentamenlade bij Wat is het verschil tussen Learning Analytics en TTL, puur omdat die ook over TTL gaat \n",
      "\n",
      "Waarop worden de links gebaseerd als iets geen tags heeft? Bv. 'zijn er nieuwe voorbeelden bij de FNWI over peer instruction?\n",
      "\n",
      "Projecten komen veelal bij elkaar omdat ze over dezelfde onderwerpen gaan, maar zijn niet per se relevant\n",
      "\n",
      "\n",
      "Glossaries of tags heeft een voorkeur voor het teruggevenv an glossaries.\n",
      "\n",
      "### 3\n",
      "3 - Wat is het verschil tussen learnin ganalytics en TTL?\n",
      "Coach (enige link) wordt door geen van allen gevonden. De text based doen het hier heel slecht en komen met allerlei zaken over stemkastjes die er niets mee te maken hebben. De tag-based approaches zijn wat dat betreft waardevoller, omdat zij bv. ook Tentamenlade aandragen. Wel onduidelijk waarom 'tentamenlade' en 'bravo' er wel bij komen maar coach niet.\n",
      "\n",
      "### GOOD PRACTICE 4 \n",
      "Gloassaries of tags vindt peer-instruction bij 4. \n",
      "\n",
      "Raar dat smoothed tags 'peer-instruction (information)' en 'peer instruction (glossaria)' niet vindt terwijl ze exact dezelfde tags hebben. \n",
      "\n",
      "Workskop Activerend college met inzet van ICT (Event), hoort die erbij?\n",
      "\n",
      "Webcolleges (UvA) (Information) door allemaal niet gevonden terwijl bij text de titels identiek zijn. Misschien sowieso meer waarde aan de titel hechten?\n",
      "\n",
      "Werkcollege met stemkastjes (Good Practice) heeft niet 'stemkastjes' als tag, whyz? Hoort 'stemkastjes' bij 'stemsysteem'?\n",
      "\n",
      "'Hoe kan ik aan stemkastjes komen? (Question)' is wel degelijk relevant, 'Shakespeak - FNWI' kan ik niet inschatten\n",
      "\n",
      "Textbased vindt 'Weblectures in AI (Good Practice)', wat gewoon goed is lijkt me. Mist zaken als 'clickers - stemkastjes' en 'webcolleges UvA'. Laatste twee missen er een hoop en komen vooral met veel irrelevante suggesties. \n",
      "\n",
      "#### 5. \n",
      "Hoe kan ik aan stemkastjes komen -> heel veel relevantie documenten die er niet handmatig ingezet zijn. Erwin van Vliet komt naar boven, moet de schrijver gelinkt zijn bij een vraag? \"Werkcollege met stemkastjes (Good Practice)\" kom ter niet in voor, deze heeft 'stemsysteem' en niet 'stemkastjes' in de titel zitten, misschien is dat relevant. Tokenizing en woorden uit elkaar trekken zou hierbij relevant kunnen zijn. De text-based approach lijken veel minder relevante resultaten terug te geven. Heel veel over tentamenlade maar waarom?\n",
      "\n",
      "### 6. \n",
      "'Zijn er nieuwe voorbeelden over peer-instruction', geen tags dus text-based lijkt het hier beter te doen, vooral als het weighted is. \n",
      "\n",
      "### 7. \n",
      "Text-based diet het hier extreem slecht. Gek dat de vraag van Sander niet omhoog komt - titel moet belangrijker worden! \n",
      "\n",
      "### PROJECT 11\n",
      "Wat is 'online schakelen'?\n",
      "Persoon bij een project waar hij niet aan gerelateerd is omdat hij wel veel tags heeft die daaraan gerelateerd zijn\n",
      "Wat the doen als iets geen tags heeft?\n",
      "Check how good suggestions are?\n",
      "Wanneer zijn projecten wel en wanneer zijn ze niet gelinkt?\n",
      "Stemkastjes - Clickers: Faculty of Science, UvA (Information) wordt wel gevonden door text vectorizers maar niet door tag vectorizers\n",
      "De weighted tag begrijpt dat ale 'clickers' relevant is 'Which cloud voting systems can I use? (Question)' met shakespeak ooh relevant is. \n",
      "Hoort 'Think-pair-share (Glossary)' erbij?\n",
      "e-Learning in Chemistry (Information)\n",
      "\n",
      "### INFORMATION 14\n",
      "Shakespeak (Glossary) is wel degelijk relevant\n",
      "\n",
      "### 20\n",
      "\"Stemsysteem\" en \"stemkastjes\" zijn met elkaar verbonden, maar dat zie thij niet - ook de gecombineerde niet. Heel veel van de dingen die terugkomen hebben wel met stemmen te maken.\n",
      "\n",
      "Texten geven, zoals te verwachten, met name NL teksten terug, die an zich wel relevant zijn. Hoe herkent hij echter dat Shakespeak er ook bij zou moeten horen? Deze lijkt overigens een stuk slechter te performer wanneer text weighted is. \n",
      "\n",
      "###26 \n",
      "Wat te doen met de persoon die iets geschreven heeft? Verder heel veel relevantie dingen die niet zo genoemd werden, al komt die glossaria wel weer met learning analytics aankakken. \n",
      "\n",
      "\n",
      "### 27 \n",
      "TPACK clearing kookboek heeft niet echt tags\n",
      "Als er geen tags zijn, vindt de text-based betere dingen - bv dingen die over e-leaning gaan. \n",
      "\n",
      "### 31 \n",
      "Is there a project document/proposal available?\n",
      "Het lijkt erop dat de glossaries of tags eenv oorkeur heeft glossaries terug te geven. - nee is niet zo\n",
      "\n",
      "### 43 \n",
      "Niet weighted en weighted geven andere dingen terug voor de text dingen. Tag doet het hier duidelijk het beste. \n",
      "\n",
      "### 48 \n",
      "Tag based doet het hier iets beter lijkt het \n",
      "\n",
      "###89 \n",
      "Webex. Is correct:\n",
      " \t- Weblectures\n",
      " \t- e-learning misschien?\n",
      "\t- Supervising Students op zich wel\n",
      " \t- Oral presentations niet,\n",
      " \t- Interactief coll\u00e8ge wel, maar al die crap van learnin analytics is niet echt boeiend (die komen overigens wel steeds terug bij glossaries of tags!). De smoothed tags lijken veel relevanter. \n",
      "\n",
      "### 90 \n",
      "Opvallend! \"90 - What is TurnitIn? (Question)\" herkent hij wel. Dit komt omdat \"TurnitIn\" wel in de beschrijving van Plagiarism control staat, maar Ithenticate niet. Interstingly zou je wel willen concluderen dat deze drie dingen van dezelfde orde zijn. \n",
      "\n",
      "### 91 \n",
      "FlippedClassroom bij Ithenticate? Valt op dat hij er hier vrijwel geen goed heeft, terwijl het de tag (die niet gegeven is\u2026) wel als tag voorkomt in de documenten. Het zou dan waardevol kunnen zijn om naar de tekst te kijken en die niet alleen aan andere inhoud van teksten maar ook aan tag glossaries te matchen. \n",
      "\n",
      "### 93 \n",
      "teaching with analogies is blijkbaar heel erg moeilijk, met name voor text-vectorizer en ook voor weighted text vectorizer. Hoort 'Lecture Breaks (Glossary)' erbij? En 'Interactief college voor grote groepen met stemsysteem en webcolleges (onderwijs herontwerp). Glossaries of tags komt met heel veel dingen over learning analytics en stemsysteemen terug, hoe kan dit?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}