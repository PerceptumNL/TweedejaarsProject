{
 "metadata": {
  "name": "",
  "signature": "sha256:84001a4cb0802c16b3eaaf6a01d02a03c03b48f2cdc0ed0d65b3b0a5e6198953"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import os, sys\n",
      "import copy\n",
      "sys.path.append(os.path.join(os.path.abspath('..'), 'src'))\n",
      "from datawrapper import DataWrapper\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "from __future__ import with_statement\n",
      "import cPickle as pickle\n",
      "import nltk\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import re\n",
      "from unidecode import unidecode\n",
      "import numpy as np\n",
      "from scipy import sparse\n",
      "import copy\n",
      "from decimal import *\n",
      "import json\n",
      "\n",
      "def find_links(data_, new_doc_idx):\n",
      "    # Remove a document from the dataset to use as 'new document'\n",
      "    data = copy.deepcopy(data_.data)\n",
      "    new_doc = data['items'][new_doc_idx]\n",
      "    \n",
      "    # Get all texts for all documents\n",
      "    texts = dict(map(lambda x: (x[0], get_text(x[0], data_)), data['items'].items()))\n",
      "    \n",
      "    # Vectorize texts\n",
      "    vectorizer = TfidfVectorizer(use_idf=True)\n",
      "    vectorizer.fit(texts.values())\n",
      "    bows = vectorizer.transform(texts.values())\n",
      "    bows = dict(zip(texts.keys(), bows))\n",
      "    \n",
      "    zero_vector = sparse.csc_matrix((1, len(vectorizer.get_feature_names())))    \n",
      "    descriptors = []\n",
      "    \n",
      "    for key in texts:\n",
      "        descriptor = bows[key] + zero_vector\n",
      "        descriptors += [(key, descriptor)]\n",
      "    \n",
      "    # Get all tags for the new document\n",
      "    new_doc_descriptor = bows[new_doc_idx] + zero_vector\n",
      "    \n",
      "    # Perform NN\n",
      "    nn = NearestNeighbors()\n",
      "    nn.fit(sparse.vstack(dict(descriptors).values()))\n",
      "    dist, idx = nn.kneighbors(new_doc_descriptor, 40)\n",
      "    \n",
      "    # Get proposed links\n",
      "    proposed_links = map(lambda x: descriptors[x][0], idx[0])\n",
      "    \n",
      "    return proposed_links\n",
      "\n",
      "def get_text(x, data_):\n",
      "    if(data_.item(x)['type'] != 'Person'):\n",
      "        text = data_.preprocessed_by_key(x, 'text')\n",
      "        title = data_.preprocessed_by_key(x, 'title')\n",
      "        return title + ' ' + text\n",
      "    else:\n",
      "        headline = data_.preprocessed_by_key(x, 'headline')\n",
      "        about = data_.preprocessed_by_key(x, 'about')\n",
      "        return headline + ' ' + about\n",
      "\n",
      "def get_title(item):\n",
      "    if(item['type'] == 'Person'):\n",
      "        return item['name']\n",
      "    else:\n",
      "        return item['title']\n",
      "\n",
      "def get_content(item, data):\n",
      "    if(item['type'] == 'Person'):\n",
      "        return item['about']\n",
      "    else:\n",
      "        author = data['items'][item['author']]['name']\n",
      "        return 'Author: ' + author + '<br><br>' + item['text']\n",
      "\n",
      "\n",
      "data = DataWrapper('../data/export_starfish_tjp.pickle')\n",
      "documents = {}\n",
      "c = 0\n",
      "# Loop through all documents\n",
      "for key in data.data['items'].keys()[1:10]:\n",
      "    # Find links of this item\n",
      "    this = data.data['items'][key]\n",
      "    links = find_links(data, item)\n",
      "    nlinks = {}\n",
      "    \n",
      "    # Bring links in proper format\n",
      "    for link in links:\n",
      "        object = data.data['items'][link]\n",
      "        title = get_title(object)\n",
      "        content = get_content(object, data.data)\n",
      "        correct = False\n",
      "        if(link in this['links']):\n",
      "            correct = True\n",
      "        nlinks[link] = ({'type': object['type'], 'title': title, 'content': content, 'correct': correct})\n",
      "      \n",
      "    # Bring current doc in proper format\n",
      "    title = get_title(this)\n",
      "    content = get_content(this, data.data)\n",
      "    doc = {'type': this['type'], 'links': nlinks, 'title': title, 'content': content}\n",
      "    documents[key] = doc\n",
      "    print('File number ' + str(c) + ' was linked')\n",
      "    c += 1\n",
      "    \n",
      "# Write objects to JSON file\n",
      "file = open(\"../src/content.json\", \"w\")\n",
      "file.write(json.dumps(documents))\n",
      "file.close()\n",
      "print('Written to file content.json')\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Usage\n",
      "\n",
      "If you want to play the visualization, run a HTTP server from the /src folder:\n",
      "\n",
      "    python -m SimpleHTTPServer\n",
      "\n",
      "In the browser, go to:\n",
      "\n",
      "    http://0.0.0.0:8000/view.html\n",
      "\n",
      "All that is generated by this notebook is saved to content.json and can be viewed at that link\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}