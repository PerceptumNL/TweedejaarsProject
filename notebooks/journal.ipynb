{
 "metadata": {
  "name": "",
  "signature": "sha256:2db25db85bc63742d9baa5643de39745968984ea40c887abc95bd7d5a807ff19"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Journal Starfish Project"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 25 & 26\n",
      "A journal post for both june 25th and june 26th as on both days we worked on the same things. During the days we received feedback from both the client and our academic mentor on our report. We used the time during the day to improve the report based on this feedback. Also we decided not to use any of the statistical models as they do not give better results. We thinks this may work better when the data set is bigger.\n",
      "\n",
      "Also on thursday slides for our final presentation were created. Also a meeting was taken with an intern of perceptum wo will work on the Starfish project during the sumer. We told him what we've learned and built during the last couple of weeks. Finally we presented our results to the client who was happy with what we could tell him."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 24\n",
      "\n",
      "Today we have worked on the report - the introduction was reviewed based on feedback of the client and we have written a big part of the method and domain sections and started with the evaluation section.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 23\n",
      "\n",
      "Today, the automated threshold was implemented in the pipeline (it was only in a notebook). We have polished the user input of the system. Also, the Bayesean weights were implemented within the pipeline. Now, the first real experiments with this implementation and the entire pipeline can be performed. We have also done some further analysis on the first vectorizers. We now have the code and notebooks that can make this analysis fast and effective. Additionally, we have discussed how to set up a proper report in such a format that we both meet the assignment requirements as the client's needs. We have also written the product overview and a few parts of the method for the report. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 20\n",
      "\n",
      "Today we have worked on the report by writing an introduction. We have also done some more qualitative analysis, but due to long processing times there has not been a lot of progress. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 19\n",
      "\n",
      "Today we have finished implementing calculating the probabilities and implemented this in the vectorizers. We have also finished the implementation of calculating the threshold values and made a start with implementing LDA. Additionally, a few new distance metrics have been tried. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 18\n",
      "\n",
      "Today we have brain stormed a lot of how to work further on the project. We have worked out the exact calculating of the tag probabilities (the chance that a given tag causes two documents of a certain type to be linked). Additionally, we have worked further on the threshold calculation. Additionally, some extra work has been done on creating a program that can deduce the vectors that were compared in NN. Now, to a certain extend, different vectors can be compared, which is necessary for qualitative analysis. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 17\n",
      "\n",
      "Today we visualised the distances returned by our algorithms to get a deeper insight for setting a threshold. We also experimented with some simple thresholds.\n",
      "\n",
      "We also worked on analyzing the performance of the current algorithms. To compare our extended tag similarities to a baseline we also implemented a simple tag frequency vectorizer.\n",
      "\n",
      "To correctly evaluate all of this, we have adopted our datawrapper to be able to create folds from the original data. This iteratively creates test and trainingsets by removes a set of documents and their links (inbound and outbound) from the data. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 16\n",
      "\n",
      "Today we have worked on processing the expert data we received from Sander and Natasa. From their descisions wheter our proposed documents were relevant, irrelevant or might be relevant we created two new datasets (one with maybe answers, one without) which we now consider as containing valid links. Lotte also processed their comments on certain data items and used that information in her qualitative analysis of our algorihtms.\n",
      "\n",
      "We have also worked on determining a threshold for our nearest neighbor algorithm, this way we return links of which we are sure they are relevant. This starts by plotting and analyzing distances for all documents when inserting a new one. This did not give direct insight to a hard threshold. \n",
      "\n",
      "Additionally, we also did some work on incorporating the directional properties of starfish into the recommender. This also takes the types of documents into account.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 13\n",
      "\n",
      "The progress presentations were scheduled today, so we spent till 12:00 on listening to other presentations and give our own presentation as well. This went quite well - we did not receive real negative feedback and our clients were also satisfied. \n",
      "\n",
      "After the presentations we finished up the implementation of tag aliasing. Then we ran the system on a new dataset - which was derived from StarFish yesterday - and created a csv file containing all links that were proposed but were not correct according to the current StarFish network. We have given this file to our client, so that he and another expert can look into it and annotate our proposals. We will use the results of this as our 'true' dataset in the upcoming week.\n",
      "\n",
      "Additionally, we have looked some more into the results of the implemented algorithms. Together with our client we agreed that it would be useful to thoroughly analise this data before we go further with developing new algorithms. We should understand why one particular algorithm gives certain results and why another algorithm behaves differently. We will look into the missed links (grey in the visualization) this weekend and the beginning of this week and once we have our new dataset we will also look in to the falsely proposed links. In particular, we will try to find a relationship between the vectors that NN uses and the preferred outcome.\n",
      "\n",
      "We have also worked out the algorithm that uses proposed links a bit more and looked a bit into LDA. However, our main priority now lies in analyzing our first results. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 12\n",
      "\n",
      "Today we have discussed a lot of ideas on how to go further. Currently, we have a lot of results, which are partially analized, but analysis is hard because for us as non-StarFish experts it is hard to decide whether a proposed link is well enough. We discussed this thoroughly with our client and decided to enhance the current knowledge base with some of the links our algorithm proposed. To do this, we created a CSV export containing all of the links our algorithms (5 of them in total now) proposed. Two experts will go through that list tomorrow to decide which links are correct and which ar not. We will add those links and use that as the 'truth' - we will not have to look into the results manually anymore (at least in evaluating the algorithms, not in improving them...)\n",
      "\n",
      "Having a network that contains proper links, we can now also use these links in the algorithm. We have discussed many ideas on how to do that today, and decided to use an extra weight on tags, namely P(X | T), the chance given a tag T that two documents are linked together (X). An additional improvement would be to take the type of the two documents that are linked in regard, e.g. change that a Question is linked to a Person given a tag T. \n",
      "\n",
      "Additionally, we want to have an approach that does not really rely on manually assigned tags, since a lot of documents have little or no tags. We decided to explore the LDA algorithm for this, to automatically assign topics. \n",
      "\n",
      "Lastly, we want to improve the number of proposed documents that we return. Now, we simply return the top 10, but we will look into a method that uses a threshold to return a proper amount of documents.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 11\n",
      "\n",
      "Today we have worked on the visualization, which can now take as input a JSON file, which makes it easier to compare the performance of different algorithms. This could be done better if we would introduce some similarity metric that can define similarity on different aspects, such as similarity in returned types ect. \n",
      "\n",
      "We have also examined the data that comes from the first 4 algorithms we have implemented. First of all, many links that are proposed seem correct to us, even though these connections do not exist in our dataset. We will discuss this tomorrow with the client, among with some other strange phenomena. However, because it is unsupervised and we are not expertises in the StarFish domain, this is quite hard. We should really think about a way of determining how the algorithm comes to certain conclusions in a fast way, we will try to implement this tomorrow before our demo (even though it is not part of the sprint). \n",
      "\n",
      "In terms of Scrum, we have finished up almost all stories of this sprint today, except for a proper automated evaluation metric. This is due to the fact that we only later on discovered that a metric that compares different approaches is more useful than one that simply checks if something is 'wrong' or 'right', since we do not know what's wrong and what's right. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# June 10\n",
      "\n",
      "Today we have worked further on the sprints. Jorn and Lotte worked on finalizing the scaffold, which now takes one of the vectorizers (= algorithm), combines it with some distance measure (e.g. cosine or eucledian distance) and generates all recommended links for all files in the dataset. The results are shown in an HTML page. We are keeping some better visualizations in mind (e.g. d3) but this works well enough for now. Robbert finished with his implementation of the weighted tags algorithm and also rewrote that one in the proper form. \n",
      "\n",
      "We also had a short meeting with our clients, in which we showed our first results. From this we have a better understanding what links between documents are correct and which are not. We also found out that our code has a few bugs, which means that one of the algorithms (that seemed to perform the best) did a totally different thing than we intended. Most of these bugs were fixed on the spot. We will analyze the results of the algorithms even further tomorrow. Additionally, we will put some work in defining a threshold for the number of recommended documents that are returned, a subject that was also discussed during the meeting with the client."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## June 09\n",
      "\n",
      "Despite the holidays we came together at the office today. First we had a stand up meeting where we discussed what we have done since our last meeting on Friday. Then we had a small meeting with our clients, where we discussed our plans for this week. We discussed the evaluation metrics and came up with some good implementable ideas. Sander asked us to make sure to have some viewable results within a few days so that we can qualitatively see how each algorithm is working. Additionally, we are going to set up an appointment with Natasa Brouwer for the expert-evaluation.\n",
      "\n",
      "After the meeting we have worked on the other sprint items. Lotte has finished the document text and weighted document text algorithm, and Robbert has been working on the weighted tags algorithm. Jorn has implemented Nearest Neighbour and has made a scaffold for the program flow of our first deliverable. However, combining the algorithms and the main program flow still requires some work, which means that we cannot fully tests the algorithms yet. Therefore, none of the sprint items can yet be annotated as done. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## June 06\n",
      "### Planning presentations\n",
      "This morning we met at 9:30 to make the last preparations for our presentation. We also prepared some questions for our meeting with our academic mentor Raquel Fernandez. From 10:30-12:30 we attended the planning presentations and also presented our own planning. The Scrum Trainer was very positive about our planning, which gives us the feeling that we are on the right track - at least from a Scrum perspective. \n",
      "\n",
      "### Meeting with academic mentor\n",
      "After the presentations we had a meeting with Raquel Fernandez, which Robrecht Jurriaans (Product Owner) also attended. We discussed our current plans from an academic point of view and brainstormed about some algorithms that might be useful for the problem at hand. Raquel agreed that the current approach of using descriptors and some comparing algorithm (such as NN) is the right way to go. Therefore, we could finalize the planning for our first sprint, leaving everything as we had planned. From this meeting we also concluded that the evaluation of the algorithms continues to be a tricky problem to tackle and we will surely be brainstorming about that next week, both with in the team as with our clients. We plan to have another meeting with Raquel next Thursday. \n",
      "\n",
      "### Working on the first stories \n",
      "We spent the rest of the day on working on the first sprint. Robbert worked on some research for the graph-based tag descriptor (story 5), Jorn set up a basic framework for the classes that the algorithms will be described in (part of story 3) and Lotte worked on the text-based tag descriptor (story 4). Monday we will really start with very first real sprint. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## June 05\n",
      "Today we have invested some time in getting to know the scrum methodology a bit more. We have rewritten our Product Backlog in User Stories. Additionally, we have written an elevator pitch, product vision and finilized the planning for sprint 1. We also prepared the presentation of tomorrow\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## June 04\n",
      "\n",
      "### Scrum Workshop and Backlog\n",
      "The day started with a scrum workshop hosted by Silverline. During this workshop we learned how to use scrum for software projects. During this workshop it became clear that it is hard for us to completely implement scrum during the project. First because we only have a small team so a real scrum master would be a bit overdone. Secondly because currently we don't completely know how to solve that problem given by the client. This means we first have to do research on the problem. Currently we are working on a first sollution that was proposed by the client. But after finishing this we will work on improving the results step by step. With this in mind we sat down with our client today to define the first items in our backlog. We defined multiple sollutions that we have have come up with as items on the backlog. Also the implementation of the scaffold software and defenition and implementation of the evaluation metric are on the backlog. For now we will try to stick to the sprint, but if we find out that these are still to limited for us to achieve something we will switch to a more kanban like process [1] as was recommended during the workshop.\n",
      "\n",
      "### Evaluation Metric\n",
      "We also took the time to take a first stab at defining a evaluation metric.\n",
      "\n",
      "\\begin{align}\n",
      "D &= \\text{Set of all documents} \\\\\n",
      "R &\\subset D = \\text{Set of all relevant documents} \\\\\n",
      "\\text{dist}(d) &= \\text{distance of document $d$} \\\\\n",
      "\\text{ndist}(d) &= \\frac{|\\text{dist}(d)|}{\\max_{\\delta \\in D}|dist(\\delta)|}\n",
      "\\end{align}\n",
      "\n",
      "With these we define the following metric to evaluate the algorithm for a single document\n",
      "\n",
      "$$ \\frac{\\frac{1}{|R|}\\sum_{d\\in R} \\text{ndist}(d)}{\\frac{1}{|\\overline{R}|}\\sum_{d\\in \\overline{R}}ndist(d)} $$\n",
      "\n",
      "We discussed these with the client and find out that a confusion matrix may be better as first evaluation metric. Therefore we will start with implementing a confusion matrix and will try to define a evaluation metric later.\n",
      "\n",
      "\n",
      "[1] Kanban. (n.d.). In Wikipedia. Retrieved June 4, 2014, from [http://en.wikipedia.org/wiki/Kanban_(development)](http://bit.ly/1pFZKER)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## June 03\n",
      "Today we finished a bare boned version of the solution proposed in the project proposal. From tommorow a more detailed journal will be writter every day for the rest of the project."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "\n",
      "\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "        margin-left:16% !important;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: Helvetica, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 130%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 22pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<IPython.core.display.HTML at 0x101f8f890>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}